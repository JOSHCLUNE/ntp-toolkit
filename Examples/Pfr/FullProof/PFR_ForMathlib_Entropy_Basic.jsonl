{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n","proof":":= by volume_tac) := Hm[μ.map X]","declId":"PFR.ForMathlib.Entropy.Basic.51_0.C2VYVgxGWlr8Qwd","decl":"/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n","proof":":= rfl","declId":"PFR.ForMathlib.Entropy.Basic.60_0.C2VYVgxGWlr8Qwd","decl":"/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n","proof":":= by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl","declId":"PFR.ForMathlib.Entropy.Basic.63_0.C2VYVgxGWlr8Qwd","decl":"/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n","proof":":= by simp [entropy]","declId":"PFR.ForMathlib.Entropy.Basic.71_0.C2VYVgxGWlr8Qwd","decl":"/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n","proof":":= by\n  rw [entropy_def, Measure.map_congr h, entropy_def]","declId":"PFR.ForMathlib.Entropy.Basic.75_0.C2VYVgxGWlr8Qwd","decl":"/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n","proof":":= measureEntropy_nonneg _","declId":"PFR.ForMathlib.Entropy.Basic.79_0.C2VYVgxGWlr8Qwd","decl":"/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n","proof":":= by\n  simp [entropy_def, h.map_eq]","declId":"PFR.ForMathlib.Entropy.Basic.82_0.C2VYVgxGWlr8Qwd","decl":"/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n","proof":":=\n  measureEntropy_le_log_card _","declId":"PFR.ForMathlib.Entropy.Basic.87_0.C2VYVgxGWlr8Qwd","decl":"/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n","proof":":= by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]","declId":"PFR.ForMathlib.Entropy.Basic.91_0.C2VYVgxGWlr8Qwd","decl":"/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n","proof":":= by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]","declId":"PFR.ForMathlib.Entropy.Basic.98_0.C2VYVgxGWlr8Qwd","decl":"/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\n","proof":":= by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]","declId":"PFR.ForMathlib.Entropy.Basic.104_0.C2VYVgxGWlr8Qwd","decl":"lemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\n","proof":":= by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]","declId":"PFR.ForMathlib.Entropy.Basic.109_0.C2VYVgxGWlr8Qwd","decl":"lemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\n","proof":":= entropy_eq_sum_finset hX hA","declId":"PFR.ForMathlib.Entropy.Basic.119_0.C2VYVgxGWlr8Qwd","decl":"lemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\n","proof":":=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)","declId":"PFR.ForMathlib.Entropy.Basic.122_0.C2VYVgxGWlr8Qwd","decl":"lemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\n","proof":":=  entropy_eq_sum_finiteRange hX","declId":"PFR.ForMathlib.Entropy.Basic.125_0.C2VYVgxGWlr8Qwd","decl":"lemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n","proof":":= by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]","declId":"PFR.ForMathlib.Entropy.Basic.128_0.C2VYVgxGWlr8Qwd","decl":"/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\n","proof":":= by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]","declId":"PFR.ForMathlib.Entropy.Basic.137_0.C2VYVgxGWlr8Qwd","decl":"lemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n","proof":":= by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]","declId":"PFR.ForMathlib.Entropy.Basic.145_0.C2VYVgxGWlr8Qwd","decl":"/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n","proof":":= by\n  simp [entropy, Measure.map_const]","declId":"PFR.ForMathlib.Entropy.Basic.155_0.C2VYVgxGWlr8Qwd","decl":"/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n","proof":":= by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl","declId":"PFR.ForMathlib.Entropy.Basic.163_0.C2VYVgxGWlr8Qwd","decl":"/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n","proof":":= by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp","declId":"PFR.ForMathlib.Entropy.Basic.186_0.C2VYVgxGWlr8Qwd","decl":"/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n","proof":":= by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp","declId":"PFR.ForMathlib.Entropy.Basic.195_0.C2VYVgxGWlr8Qwd","decl":"/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n","proof":":= by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg","declId":"PFR.ForMathlib.Entropy.Basic.208_0.C2VYVgxGWlr8Qwd","decl":"/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n","proof":":= by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs","declId":"PFR.ForMathlib.Entropy.Basic.301_0.C2VYVgxGWlr8Qwd","decl":"/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n","proof":":= by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]","declId":"PFR.ForMathlib.Entropy.Basic.313_0.C2VYVgxGWlr8Qwd","decl":"/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n","proof":":= by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective","declId":"PFR.ForMathlib.Entropy.Basic.326_0.C2VYVgxGWlr8Qwd","decl":"/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n","proof":":= by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _","declId":"PFR.ForMathlib.Entropy.Basic.332_0.C2VYVgxGWlr8Qwd","decl":"/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n","proof":":=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1","declId":"PFR.ForMathlib.Entropy.Basic.338_0.C2VYVgxGWlr8Qwd","decl":"/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n","proof":":= by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]","declId":"PFR.ForMathlib.Entropy.Basic.348_0.C2VYVgxGWlr8Qwd","decl":"/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\n","proof":":= rfl","declId":"PFR.ForMathlib.Entropy.Basic.355_0.C2VYVgxGWlr8Qwd","decl":"lemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\n","proof":":= by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht","declId":"PFR.ForMathlib.Entropy.Basic.361_0.C2VYVgxGWlr8Qwd","decl":"lemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n","proof":":= by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]","declId":"PFR.ForMathlib.Entropy.Basic.369_0.C2VYVgxGWlr8Qwd","decl":"/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n","proof":":= by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht","declId":"PFR.ForMathlib.Entropy.Basic.384_0.C2VYVgxGWlr8Qwd","decl":"/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\n","proof":":= by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]","declId":"PFR.ForMathlib.Entropy.Basic.395_0.C2VYVgxGWlr8Qwd","decl":"lemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n","proof":":=\n  by simp [condEntropy]","declId":"PFR.ForMathlib.Entropy.Basic.408_0.C2VYVgxGWlr8Qwd","decl":"/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n","proof":":=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)","declId":"PFR.ForMathlib.Entropy.Basic.413_0.C2VYVgxGWlr8Qwd","decl":"/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n","proof":":= by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp","declId":"PFR.ForMathlib.Entropy.Basic.417_0.C2VYVgxGWlr8Qwd","decl":"/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n","proof":":= by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY","declId":"PFR.ForMathlib.Entropy.Basic.427_0.C2VYVgxGWlr8Qwd","decl":"/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n","proof":":= by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]","declId":"PFR.ForMathlib.Entropy.Basic.435_0.C2VYVgxGWlr8Qwd","decl":"/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\n","proof":":= by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]","declId":"PFR.ForMathlib.Entropy.Basic.443_0.C2VYVgxGWlr8Qwd","decl":"lemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n","proof":":= by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]","declId":"PFR.ForMathlib.Entropy.Basic.465_0.C2VYVgxGWlr8Qwd","decl":"/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n","proof":":= by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]","declId":"PFR.ForMathlib.Entropy.Basic.474_0.C2VYVgxGWlr8Qwd","decl":"/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n","proof":":= by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]","declId":"PFR.ForMathlib.Entropy.Basic.482_0.C2VYVgxGWlr8Qwd","decl":"/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n","proof":":=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))","declId":"PFR.ForMathlib.Entropy.Basic.504_0.C2VYVgxGWlr8Qwd","decl":"/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n","proof":":= by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm","declId":"PFR.ForMathlib.Entropy.Basic.510_0.C2VYVgxGWlr8Qwd","decl":"/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n","proof":":= by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _","declId":"PFR.ForMathlib.Entropy.Basic.521_0.C2VYVgxGWlr8Qwd","decl":"/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n","proof":":= by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]","declId":"PFR.ForMathlib.Entropy.Basic.547_0.C2VYVgxGWlr8Qwd","decl":"/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n","proof":":= by\n  rw [chain_rule μ hX hY, add_sub_cancel']","declId":"PFR.ForMathlib.Entropy.Basic.553_0.C2VYVgxGWlr8Qwd","decl":"/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n","proof":":= by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]","declId":"PFR.ForMathlib.Entropy.Basic.559_0.C2VYVgxGWlr8Qwd","decl":"/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n","proof":":= by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf","declId":"PFR.ForMathlib.Entropy.Basic.570_0.C2VYVgxGWlr8Qwd","decl":"/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n","proof":":= by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]","declId":"PFR.ForMathlib.Entropy.Basic.583_0.C2VYVgxGWlr8Qwd","decl":"/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n","proof":":= by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)","declId":"PFR.ForMathlib.Entropy.Basic.588_0.C2VYVgxGWlr8Qwd","decl":"/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n","proof":":= by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]","declId":"PFR.ForMathlib.Entropy.Basic.602_0.C2VYVgxGWlr8Qwd","decl":"/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n","proof":":= by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ","declId":"PFR.ForMathlib.Entropy.Basic.608_0.C2VYVgxGWlr8Qwd","decl":"/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n","proof":":= by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith","declId":"PFR.ForMathlib.Entropy.Basic.623_0.C2VYVgxGWlr8Qwd","decl":"/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n","proof":":= by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]","declId":"PFR.ForMathlib.Entropy.Basic.640_0.C2VYVgxGWlr8Qwd","decl":"/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\n","proof":":= rfl","declId":"PFR.ForMathlib.Entropy.Basic.648_0.C2VYVgxGWlr8Qwd","decl":"lemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n","proof":":= by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]","declId":"PFR.ForMathlib.Entropy.Basic.651_0.C2VYVgxGWlr8Qwd","decl":"/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\n","proof":":= sub_sub_self _ _","declId":"PFR.ForMathlib.Entropy.Basic.655_0.C2VYVgxGWlr8Qwd","decl":"lemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n","proof":":= by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel","declId":"PFR.ForMathlib.Entropy.Basic.658_0.C2VYVgxGWlr8Qwd","decl":"/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n","proof":":= by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]","declId":"PFR.ForMathlib.Entropy.Basic.665_0.C2VYVgxGWlr8Qwd","decl":"/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n","proof":":= by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]","declId":"PFR.ForMathlib.Entropy.Basic.671_0.C2VYVgxGWlr8Qwd","decl":"/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n","proof":":= by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]","declId":"PFR.ForMathlib.Entropy.Basic.677_0.C2VYVgxGWlr8Qwd","decl":"/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n","proof":":= by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg","declId":"PFR.ForMathlib.Entropy.Basic.683_0.C2VYVgxGWlr8Qwd","decl":"/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n","proof":":= by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]","declId":"PFR.ForMathlib.Entropy.Basic.697_0.C2VYVgxGWlr8Qwd","decl":"/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n","proof":":=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _","declId":"PFR.ForMathlib.Entropy.Basic.705_0.C2VYVgxGWlr8Qwd","decl":"/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n","proof":":= by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)","declId":"PFR.ForMathlib.Entropy.Basic.710_0.C2VYVgxGWlr8Qwd","decl":"/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\n","proof":":= mutualInfo_eq_zero","declId":"PFR.ForMathlib.Entropy.Basic.733_0.C2VYVgxGWlr8Qwd","decl":"protected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n","proof":":= (indepFun_const c).mutualInfo_eq_zero hX measurable_const","declId":"PFR.ForMathlib.Entropy.Basic.735_0.C2VYVgxGWlr8Qwd","decl":"/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 := (indepFun_const c).mutualInfo_eq_zero hX measurable_const\n\n","proof":":= by\n  have := h.mutualInfo_eq_zero hX hY\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY] at this\n  linarith","declId":"PFR.ForMathlib.Entropy.Basic.739_0.C2VYVgxGWlr8Qwd","decl":"lemma IndepFun.condEntropy_eq_entropy {μ : Measure Ω} (h : IndepFun X Y μ)\n    (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[X ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 := (indepFun_const c).mutualInfo_eq_zero hX measurable_const\n\nlemma IndepFun.condEntropy_eq_entropy {μ : Measure Ω} (h : IndepFun X Y μ)\n    (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[X ; μ] := by\n  have := h.mutualInfo_eq_zero hX hY\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY] at this\n  linarith\n\n","proof":":= by\n  rw [eq_comm, ← sub_eq_zero, ← mutualInfo_eq_zero hX hY]; rfl","declId":"PFR.ForMathlib.Entropy.Basic.746_0.C2VYVgxGWlr8Qwd","decl":"/-- $H[X, Y] = H[X] + H[Y]$ if and only if $X, Y$ are independent. -/\nlemma entropy_pair_eq_add (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y ; μ] ↔ IndepFun X Y μ "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 := (indepFun_const c).mutualInfo_eq_zero hX measurable_const\n\nlemma IndepFun.condEntropy_eq_entropy {μ : Measure Ω} (h : IndepFun X Y μ)\n    (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[X ; μ] := by\n  have := h.mutualInfo_eq_zero hX hY\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY] at this\n  linarith\n\n/-- $H[X, Y] = H[X] + H[Y]$ if and only if $X, Y$ are independent. -/\nlemma entropy_pair_eq_add (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y ; μ] ↔ IndepFun X Y μ := by\n  rw [eq_comm, ← sub_eq_zero, ← mutualInfo_eq_zero hX hY]; rfl\n\n","proof":":= entropy_pair_eq_add","declId":"PFR.ForMathlib.Entropy.Basic.752_0.C2VYVgxGWlr8Qwd","decl":"/-- If $X, Y$ are independent, then $H[X, Y] = H[X] + H[Y]$. -/\nprotected alias ⟨_, IndepFun.entropy_pair_eq_add⟩ "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 := (indepFun_const c).mutualInfo_eq_zero hX measurable_const\n\nlemma IndepFun.condEntropy_eq_entropy {μ : Measure Ω} (h : IndepFun X Y μ)\n    (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[X ; μ] := by\n  have := h.mutualInfo_eq_zero hX hY\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY] at this\n  linarith\n\n/-- $H[X, Y] = H[X] + H[Y]$ if and only if $X, Y$ are independent. -/\nlemma entropy_pair_eq_add (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y ; μ] ↔ IndepFun X Y μ := by\n  rw [eq_comm, ← sub_eq_zero, ← mutualInfo_eq_zero hX hY]; rfl\n\n/-- If $X, Y$ are independent, then $H[X, Y] = H[X] + H[Y]$. -/\nprotected alias ⟨_, IndepFun.entropy_pair_eq_add⟩ := entropy_pair_eq_add\n\n","proof":":= by volume_tac) :\n    ℝ := (μ.map Z)[fun z ↦ H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]]","declId":"PFR.ForMathlib.Entropy.Basic.755_0.C2VYVgxGWlr8Qwd","decl":"/-- The conditional mutual information $I[X : Y| Z]$ is the mutual information of $X| Z=z$ and\n$Y| Z=z$, integrated over $z$. -/\nnoncomputable\ndef condMutualInfo (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 := (indepFun_const c).mutualInfo_eq_zero hX measurable_const\n\nlemma IndepFun.condEntropy_eq_entropy {μ : Measure Ω} (h : IndepFun X Y μ)\n    (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[X ; μ] := by\n  have := h.mutualInfo_eq_zero hX hY\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY] at this\n  linarith\n\n/-- $H[X, Y] = H[X] + H[Y]$ if and only if $X, Y$ are independent. -/\nlemma entropy_pair_eq_add (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y ; μ] ↔ IndepFun X Y μ := by\n  rw [eq_comm, ← sub_eq_zero, ← mutualInfo_eq_zero hX hY]; rfl\n\n/-- If $X, Y$ are independent, then $H[X, Y] = H[X] + H[Y]$. -/\nprotected alias ⟨_, IndepFun.entropy_pair_eq_add⟩ := entropy_pair_eq_add\n\n/-- The conditional mutual information $I[X : Y| Z]$ is the mutual information of $X| Z=z$ and\n$Y| Z=z$, integrated over $z$. -/\nnoncomputable\ndef condMutualInfo (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω := by volume_tac) :\n    ℝ := (μ.map Z)[fun z ↦ H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]]\n\n","proof":":= rfl","declId":"PFR.ForMathlib.Entropy.Basic.761_0.C2VYVgxGWlr8Qwd","decl":"lemma condMutualInfo_def (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω) :\n    condMutualInfo X Y Z μ = (μ.map Z)[fun z ↦\n      H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 := (indepFun_const c).mutualInfo_eq_zero hX measurable_const\n\nlemma IndepFun.condEntropy_eq_entropy {μ : Measure Ω} (h : IndepFun X Y μ)\n    (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[X ; μ] := by\n  have := h.mutualInfo_eq_zero hX hY\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY] at this\n  linarith\n\n/-- $H[X, Y] = H[X] + H[Y]$ if and only if $X, Y$ are independent. -/\nlemma entropy_pair_eq_add (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y ; μ] ↔ IndepFun X Y μ := by\n  rw [eq_comm, ← sub_eq_zero, ← mutualInfo_eq_zero hX hY]; rfl\n\n/-- If $X, Y$ are independent, then $H[X, Y] = H[X] + H[Y]$. -/\nprotected alias ⟨_, IndepFun.entropy_pair_eq_add⟩ := entropy_pair_eq_add\n\n/-- The conditional mutual information $I[X : Y| Z]$ is the mutual information of $X| Z=z$ and\n$Y| Z=z$, integrated over $z$. -/\nnoncomputable\ndef condMutualInfo (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω := by volume_tac) :\n    ℝ := (μ.map Z)[fun z ↦ H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]]\n\nlemma condMutualInfo_def (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω) :\n    condMutualInfo X Y Z μ = (μ.map Z)[fun z ↦\n      H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]] := rfl\n\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \";\" μ \"]\" => condMutualInfo X Y Z μ\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \"]\" => condMutualInfo X Y Z volume\n\n","proof":":= by\n  rcases finiteSupport_of_finiteRange (μ := μ) (X := Z) with ⟨A, hA⟩\n  simp_rw [condMutualInfo_def, entropy_def, kernel.mutualInfo, kernel.entropy,\n    integral_eq_sum' _ hA, smul_eq_mul, mul_sub, mul_add, Finset.sum_sub_distrib, Finset.sum_add_distrib]\n  congr with x\n  · have h := condDistrib_fst_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hX hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · have h := condDistrib_snd_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hY hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [condDistrib_apply (hX.prod_mk hY) hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx","declId":"PFR.ForMathlib.Entropy.Basic.770_0.C2VYVgxGWlr8Qwd","decl":"/-- The conditional mutual information agrees with the information of the conditional kernel.\n-/\nlemma condMutualInfo_eq_kernel_mutualInfo\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = Ik[condDistrib (⟨X, Y⟩) Z μ, μ.map Z] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 := (indepFun_const c).mutualInfo_eq_zero hX measurable_const\n\nlemma IndepFun.condEntropy_eq_entropy {μ : Measure Ω} (h : IndepFun X Y μ)\n    (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[X ; μ] := by\n  have := h.mutualInfo_eq_zero hX hY\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY] at this\n  linarith\n\n/-- $H[X, Y] = H[X] + H[Y]$ if and only if $X, Y$ are independent. -/\nlemma entropy_pair_eq_add (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y ; μ] ↔ IndepFun X Y μ := by\n  rw [eq_comm, ← sub_eq_zero, ← mutualInfo_eq_zero hX hY]; rfl\n\n/-- If $X, Y$ are independent, then $H[X, Y] = H[X] + H[Y]$. -/\nprotected alias ⟨_, IndepFun.entropy_pair_eq_add⟩ := entropy_pair_eq_add\n\n/-- The conditional mutual information $I[X : Y| Z]$ is the mutual information of $X| Z=z$ and\n$Y| Z=z$, integrated over $z$. -/\nnoncomputable\ndef condMutualInfo (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω := by volume_tac) :\n    ℝ := (μ.map Z)[fun z ↦ H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]]\n\nlemma condMutualInfo_def (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω) :\n    condMutualInfo X Y Z μ = (μ.map Z)[fun z ↦\n      H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]] := rfl\n\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \";\" μ \"]\" => condMutualInfo X Y Z μ\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \"]\" => condMutualInfo X Y Z volume\n\n/-- The conditional mutual information agrees with the information of the conditional kernel.\n-/\nlemma condMutualInfo_eq_kernel_mutualInfo\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = Ik[condDistrib (⟨X, Y⟩) Z μ, μ.map Z] := by\n  rcases finiteSupport_of_finiteRange (μ := μ) (X := Z) with ⟨A, hA⟩\n  simp_rw [condMutualInfo_def, entropy_def, kernel.mutualInfo, kernel.entropy,\n    integral_eq_sum' _ hA, smul_eq_mul, mul_sub, mul_add, Finset.sum_sub_distrib, Finset.sum_add_distrib]\n  congr with x\n  · have h := condDistrib_fst_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hX hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · have h := condDistrib_snd_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hY hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [condDistrib_apply (hX.prod_mk hY) hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n\n","proof":":= rfl","declId":"PFR.ForMathlib.Entropy.Basic.799_0.C2VYVgxGWlr8Qwd","decl":"lemma condMutualInfo_eq_integral_mutualInfo :\n    I[X : Y | Z ; μ] = (μ.map Z)[fun z ↦ I[X : Y ; μ[| Z ⁻¹' {z}]]] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 := (indepFun_const c).mutualInfo_eq_zero hX measurable_const\n\nlemma IndepFun.condEntropy_eq_entropy {μ : Measure Ω} (h : IndepFun X Y μ)\n    (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[X ; μ] := by\n  have := h.mutualInfo_eq_zero hX hY\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY] at this\n  linarith\n\n/-- $H[X, Y] = H[X] + H[Y]$ if and only if $X, Y$ are independent. -/\nlemma entropy_pair_eq_add (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y ; μ] ↔ IndepFun X Y μ := by\n  rw [eq_comm, ← sub_eq_zero, ← mutualInfo_eq_zero hX hY]; rfl\n\n/-- If $X, Y$ are independent, then $H[X, Y] = H[X] + H[Y]$. -/\nprotected alias ⟨_, IndepFun.entropy_pair_eq_add⟩ := entropy_pair_eq_add\n\n/-- The conditional mutual information $I[X : Y| Z]$ is the mutual information of $X| Z=z$ and\n$Y| Z=z$, integrated over $z$. -/\nnoncomputable\ndef condMutualInfo (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω := by volume_tac) :\n    ℝ := (μ.map Z)[fun z ↦ H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]]\n\nlemma condMutualInfo_def (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω) :\n    condMutualInfo X Y Z μ = (μ.map Z)[fun z ↦\n      H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]] := rfl\n\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \";\" μ \"]\" => condMutualInfo X Y Z μ\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \"]\" => condMutualInfo X Y Z volume\n\n/-- The conditional mutual information agrees with the information of the conditional kernel.\n-/\nlemma condMutualInfo_eq_kernel_mutualInfo\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = Ik[condDistrib (⟨X, Y⟩) Z μ, μ.map Z] := by\n  rcases finiteSupport_of_finiteRange (μ := μ) (X := Z) with ⟨A, hA⟩\n  simp_rw [condMutualInfo_def, entropy_def, kernel.mutualInfo, kernel.entropy,\n    integral_eq_sum' _ hA, smul_eq_mul, mul_sub, mul_add, Finset.sum_sub_distrib, Finset.sum_add_distrib]\n  congr with x\n  · have h := condDistrib_fst_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hX hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · have h := condDistrib_snd_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hY hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [condDistrib_apply (hX.prod_mk hY) hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n\nlemma condMutualInfo_eq_integral_mutualInfo :\n    I[X : Y | Z ; μ] = (μ.map Z)[fun z ↦ I[X : Y ; μ[| Z ⁻¹' {z}]]] := rfl\n\n","proof":":= by\n  rw [condMutualInfo_eq_integral_mutualInfo, integral_eq_sum' _ (FiniteRange.null_of_compl _ Z)]\n  congr 1 with z\n  rw [map_apply hZ (MeasurableSet.singleton z)]\n  rfl","declId":"PFR.ForMathlib.Entropy.Basic.802_0.C2VYVgxGWlr8Qwd","decl":"lemma condMutualInfo_eq_sum [IsFiniteMeasure μ] (hZ : Measurable Z) [FiniteRange Z] :\n    I[X : Y | Z ; μ] = ∑ z in FiniteRange.toFinset Z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 := (indepFun_const c).mutualInfo_eq_zero hX measurable_const\n\nlemma IndepFun.condEntropy_eq_entropy {μ : Measure Ω} (h : IndepFun X Y μ)\n    (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[X ; μ] := by\n  have := h.mutualInfo_eq_zero hX hY\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY] at this\n  linarith\n\n/-- $H[X, Y] = H[X] + H[Y]$ if and only if $X, Y$ are independent. -/\nlemma entropy_pair_eq_add (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y ; μ] ↔ IndepFun X Y μ := by\n  rw [eq_comm, ← sub_eq_zero, ← mutualInfo_eq_zero hX hY]; rfl\n\n/-- If $X, Y$ are independent, then $H[X, Y] = H[X] + H[Y]$. -/\nprotected alias ⟨_, IndepFun.entropy_pair_eq_add⟩ := entropy_pair_eq_add\n\n/-- The conditional mutual information $I[X : Y| Z]$ is the mutual information of $X| Z=z$ and\n$Y| Z=z$, integrated over $z$. -/\nnoncomputable\ndef condMutualInfo (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω := by volume_tac) :\n    ℝ := (μ.map Z)[fun z ↦ H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]]\n\nlemma condMutualInfo_def (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω) :\n    condMutualInfo X Y Z μ = (μ.map Z)[fun z ↦\n      H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]] := rfl\n\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \";\" μ \"]\" => condMutualInfo X Y Z μ\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \"]\" => condMutualInfo X Y Z volume\n\n/-- The conditional mutual information agrees with the information of the conditional kernel.\n-/\nlemma condMutualInfo_eq_kernel_mutualInfo\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = Ik[condDistrib (⟨X, Y⟩) Z μ, μ.map Z] := by\n  rcases finiteSupport_of_finiteRange (μ := μ) (X := Z) with ⟨A, hA⟩\n  simp_rw [condMutualInfo_def, entropy_def, kernel.mutualInfo, kernel.entropy,\n    integral_eq_sum' _ hA, smul_eq_mul, mul_sub, mul_add, Finset.sum_sub_distrib, Finset.sum_add_distrib]\n  congr with x\n  · have h := condDistrib_fst_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hX hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · have h := condDistrib_snd_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hY hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [condDistrib_apply (hX.prod_mk hY) hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n\nlemma condMutualInfo_eq_integral_mutualInfo :\n    I[X : Y | Z ; μ] = (μ.map Z)[fun z ↦ I[X : Y ; μ[| Z ⁻¹' {z}]]] := rfl\n\nlemma condMutualInfo_eq_sum [IsFiniteMeasure μ] (hZ : Measurable Z) [FiniteRange Z] :\n    I[X : Y | Z ; μ] = ∑ z in FiniteRange.toFinset Z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_integral_mutualInfo, integral_eq_sum' _ (FiniteRange.null_of_compl _ Z)]\n  congr 1 with z\n  rw [map_apply hZ (MeasurableSet.singleton z)]\n  rfl\n\n","proof":":= by\n  rw [condMutualInfo_eq_sum hZ]\n  apply Finset.sum_subset\n  . simp\n  intro z _ hz\n  have : Z ⁻¹' {z} = ∅ := by\n    ext ω\n    simp at hz\n    simp [hz]\n  simp [this]","declId":"PFR.ForMathlib.Entropy.Basic.809_0.C2VYVgxGWlr8Qwd","decl":"/-- A variant of `condMutualInfo_eq_sum` when `Z` has finite codomain. -/\nlemma condMutualInfo_eq_sum' [IsFiniteMeasure μ] (hZ : Measurable Z) [Fintype U] :\n    I[X : Y | Z ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 := (indepFun_const c).mutualInfo_eq_zero hX measurable_const\n\nlemma IndepFun.condEntropy_eq_entropy {μ : Measure Ω} (h : IndepFun X Y μ)\n    (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[X ; μ] := by\n  have := h.mutualInfo_eq_zero hX hY\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY] at this\n  linarith\n\n/-- $H[X, Y] = H[X] + H[Y]$ if and only if $X, Y$ are independent. -/\nlemma entropy_pair_eq_add (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y ; μ] ↔ IndepFun X Y μ := by\n  rw [eq_comm, ← sub_eq_zero, ← mutualInfo_eq_zero hX hY]; rfl\n\n/-- If $X, Y$ are independent, then $H[X, Y] = H[X] + H[Y]$. -/\nprotected alias ⟨_, IndepFun.entropy_pair_eq_add⟩ := entropy_pair_eq_add\n\n/-- The conditional mutual information $I[X : Y| Z]$ is the mutual information of $X| Z=z$ and\n$Y| Z=z$, integrated over $z$. -/\nnoncomputable\ndef condMutualInfo (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω := by volume_tac) :\n    ℝ := (μ.map Z)[fun z ↦ H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]]\n\nlemma condMutualInfo_def (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω) :\n    condMutualInfo X Y Z μ = (μ.map Z)[fun z ↦\n      H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]] := rfl\n\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \";\" μ \"]\" => condMutualInfo X Y Z μ\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \"]\" => condMutualInfo X Y Z volume\n\n/-- The conditional mutual information agrees with the information of the conditional kernel.\n-/\nlemma condMutualInfo_eq_kernel_mutualInfo\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = Ik[condDistrib (⟨X, Y⟩) Z μ, μ.map Z] := by\n  rcases finiteSupport_of_finiteRange (μ := μ) (X := Z) with ⟨A, hA⟩\n  simp_rw [condMutualInfo_def, entropy_def, kernel.mutualInfo, kernel.entropy,\n    integral_eq_sum' _ hA, smul_eq_mul, mul_sub, mul_add, Finset.sum_sub_distrib, Finset.sum_add_distrib]\n  congr with x\n  · have h := condDistrib_fst_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hX hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · have h := condDistrib_snd_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hY hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [condDistrib_apply (hX.prod_mk hY) hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n\nlemma condMutualInfo_eq_integral_mutualInfo :\n    I[X : Y | Z ; μ] = (μ.map Z)[fun z ↦ I[X : Y ; μ[| Z ⁻¹' {z}]]] := rfl\n\nlemma condMutualInfo_eq_sum [IsFiniteMeasure μ] (hZ : Measurable Z) [FiniteRange Z] :\n    I[X : Y | Z ; μ] = ∑ z in FiniteRange.toFinset Z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_integral_mutualInfo, integral_eq_sum' _ (FiniteRange.null_of_compl _ Z)]\n  congr 1 with z\n  rw [map_apply hZ (MeasurableSet.singleton z)]\n  rfl\n\n/-- A variant of `condMutualInfo_eq_sum` when `Z` has finite codomain. -/\nlemma condMutualInfo_eq_sum' [IsFiniteMeasure μ] (hZ : Measurable Z) [Fintype U] :\n    I[X : Y | Z ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_sum hZ]\n  apply Finset.sum_subset\n  . simp\n  intro z _ hz\n  have : Z ⁻¹' {z} = ∅ := by\n    ext ω\n    simp at hz\n    simp [hz]\n  simp [this]\n\n","proof":":= by\n  simp_rw [condMutualInfo_def, add_comm, entropy_comm hX hY]","declId":"PFR.ForMathlib.Entropy.Basic.822_0.C2VYVgxGWlr8Qwd","decl":"/-- $I[X : Y | Z] = I[Y : X | Z]$. -/\nlemma condMutualInfo_comm\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω) :\n    I[X : Y | Z ; μ] = I[Y : X | Z ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 := (indepFun_const c).mutualInfo_eq_zero hX measurable_const\n\nlemma IndepFun.condEntropy_eq_entropy {μ : Measure Ω} (h : IndepFun X Y μ)\n    (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[X ; μ] := by\n  have := h.mutualInfo_eq_zero hX hY\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY] at this\n  linarith\n\n/-- $H[X, Y] = H[X] + H[Y]$ if and only if $X, Y$ are independent. -/\nlemma entropy_pair_eq_add (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y ; μ] ↔ IndepFun X Y μ := by\n  rw [eq_comm, ← sub_eq_zero, ← mutualInfo_eq_zero hX hY]; rfl\n\n/-- If $X, Y$ are independent, then $H[X, Y] = H[X] + H[Y]$. -/\nprotected alias ⟨_, IndepFun.entropy_pair_eq_add⟩ := entropy_pair_eq_add\n\n/-- The conditional mutual information $I[X : Y| Z]$ is the mutual information of $X| Z=z$ and\n$Y| Z=z$, integrated over $z$. -/\nnoncomputable\ndef condMutualInfo (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω := by volume_tac) :\n    ℝ := (μ.map Z)[fun z ↦ H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]]\n\nlemma condMutualInfo_def (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω) :\n    condMutualInfo X Y Z μ = (μ.map Z)[fun z ↦\n      H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]] := rfl\n\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \";\" μ \"]\" => condMutualInfo X Y Z μ\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \"]\" => condMutualInfo X Y Z volume\n\n/-- The conditional mutual information agrees with the information of the conditional kernel.\n-/\nlemma condMutualInfo_eq_kernel_mutualInfo\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = Ik[condDistrib (⟨X, Y⟩) Z μ, μ.map Z] := by\n  rcases finiteSupport_of_finiteRange (μ := μ) (X := Z) with ⟨A, hA⟩\n  simp_rw [condMutualInfo_def, entropy_def, kernel.mutualInfo, kernel.entropy,\n    integral_eq_sum' _ hA, smul_eq_mul, mul_sub, mul_add, Finset.sum_sub_distrib, Finset.sum_add_distrib]\n  congr with x\n  · have h := condDistrib_fst_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hX hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · have h := condDistrib_snd_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hY hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [condDistrib_apply (hX.prod_mk hY) hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n\nlemma condMutualInfo_eq_integral_mutualInfo :\n    I[X : Y | Z ; μ] = (μ.map Z)[fun z ↦ I[X : Y ; μ[| Z ⁻¹' {z}]]] := rfl\n\nlemma condMutualInfo_eq_sum [IsFiniteMeasure μ] (hZ : Measurable Z) [FiniteRange Z] :\n    I[X : Y | Z ; μ] = ∑ z in FiniteRange.toFinset Z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_integral_mutualInfo, integral_eq_sum' _ (FiniteRange.null_of_compl _ Z)]\n  congr 1 with z\n  rw [map_apply hZ (MeasurableSet.singleton z)]\n  rfl\n\n/-- A variant of `condMutualInfo_eq_sum` when `Z` has finite codomain. -/\nlemma condMutualInfo_eq_sum' [IsFiniteMeasure μ] (hZ : Measurable Z) [Fintype U] :\n    I[X : Y | Z ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_sum hZ]\n  apply Finset.sum_subset\n  . simp\n  intro z _ hz\n  have : Z ⁻¹' {z} = ∅ := by\n    ext ω\n    simp at hz\n    simp [hz]\n  simp [this]\n\n/-- $I[X : Y | Z] = I[Y : X | Z]$. -/\nlemma condMutualInfo_comm\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω) :\n    I[X : Y | Z ; μ] = I[Y : X | Z ; μ] := by\n  simp_rw [condMutualInfo_def, add_comm, entropy_comm hX hY]\n\n","proof":":= by\n  refine integral_nonneg (fun z ↦ ?_)\n  exact mutualInfo_nonneg hX hY _","declId":"PFR.ForMathlib.Entropy.Basic.828_0.C2VYVgxGWlr8Qwd","decl":"/-- Conditional information is non-nonegative. -/\nlemma condMutualInfo_nonneg\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y | Z ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 := (indepFun_const c).mutualInfo_eq_zero hX measurable_const\n\nlemma IndepFun.condEntropy_eq_entropy {μ : Measure Ω} (h : IndepFun X Y μ)\n    (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[X ; μ] := by\n  have := h.mutualInfo_eq_zero hX hY\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY] at this\n  linarith\n\n/-- $H[X, Y] = H[X] + H[Y]$ if and only if $X, Y$ are independent. -/\nlemma entropy_pair_eq_add (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y ; μ] ↔ IndepFun X Y μ := by\n  rw [eq_comm, ← sub_eq_zero, ← mutualInfo_eq_zero hX hY]; rfl\n\n/-- If $X, Y$ are independent, then $H[X, Y] = H[X] + H[Y]$. -/\nprotected alias ⟨_, IndepFun.entropy_pair_eq_add⟩ := entropy_pair_eq_add\n\n/-- The conditional mutual information $I[X : Y| Z]$ is the mutual information of $X| Z=z$ and\n$Y| Z=z$, integrated over $z$. -/\nnoncomputable\ndef condMutualInfo (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω := by volume_tac) :\n    ℝ := (μ.map Z)[fun z ↦ H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]]\n\nlemma condMutualInfo_def (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω) :\n    condMutualInfo X Y Z μ = (μ.map Z)[fun z ↦\n      H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]] := rfl\n\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \";\" μ \"]\" => condMutualInfo X Y Z μ\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \"]\" => condMutualInfo X Y Z volume\n\n/-- The conditional mutual information agrees with the information of the conditional kernel.\n-/\nlemma condMutualInfo_eq_kernel_mutualInfo\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = Ik[condDistrib (⟨X, Y⟩) Z μ, μ.map Z] := by\n  rcases finiteSupport_of_finiteRange (μ := μ) (X := Z) with ⟨A, hA⟩\n  simp_rw [condMutualInfo_def, entropy_def, kernel.mutualInfo, kernel.entropy,\n    integral_eq_sum' _ hA, smul_eq_mul, mul_sub, mul_add, Finset.sum_sub_distrib, Finset.sum_add_distrib]\n  congr with x\n  · have h := condDistrib_fst_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hX hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · have h := condDistrib_snd_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hY hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [condDistrib_apply (hX.prod_mk hY) hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n\nlemma condMutualInfo_eq_integral_mutualInfo :\n    I[X : Y | Z ; μ] = (μ.map Z)[fun z ↦ I[X : Y ; μ[| Z ⁻¹' {z}]]] := rfl\n\nlemma condMutualInfo_eq_sum [IsFiniteMeasure μ] (hZ : Measurable Z) [FiniteRange Z] :\n    I[X : Y | Z ; μ] = ∑ z in FiniteRange.toFinset Z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_integral_mutualInfo, integral_eq_sum' _ (FiniteRange.null_of_compl _ Z)]\n  congr 1 with z\n  rw [map_apply hZ (MeasurableSet.singleton z)]\n  rfl\n\n/-- A variant of `condMutualInfo_eq_sum` when `Z` has finite codomain. -/\nlemma condMutualInfo_eq_sum' [IsFiniteMeasure μ] (hZ : Measurable Z) [Fintype U] :\n    I[X : Y | Z ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_sum hZ]\n  apply Finset.sum_subset\n  . simp\n  intro z _ hz\n  have : Z ⁻¹' {z} = ∅ := by\n    ext ω\n    simp at hz\n    simp [hz]\n  simp [this]\n\n/-- $I[X : Y | Z] = I[Y : X | Z]$. -/\nlemma condMutualInfo_comm\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω) :\n    I[X : Y | Z ; μ] = I[Y : X | Z ; μ] := by\n  simp_rw [condMutualInfo_def, add_comm, entropy_comm hX hY]\n\n/-- Conditional information is non-nonegative. -/\nlemma condMutualInfo_nonneg\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y | Z ; μ] := by\n  refine integral_nonneg (fun z ↦ ?_)\n  exact mutualInfo_nonneg hX hY _\n\n","proof":":= by\n  rw [condMutualInfo_eq_kernel_mutualInfo hX hY hZ, kernel.mutualInfo,\n    kernel.entropy_congr (condDistrib_fst_ae_eq hX hY hZ _),\n    kernel.entropy_congr (condDistrib_snd_ae_eq hX hY hZ _),\n    condEntropy_eq_kernel_entropy hX hZ, condEntropy_eq_kernel_entropy hY hZ,\n    condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ]","declId":"PFR.ForMathlib.Entropy.Basic.836_0.C2VYVgxGWlr8Qwd","decl":"/-- $$ I[X : Y| Z] = H[X| Z] + H[Y| Z] - H[X, Y| Z].$$ -/\nlemma condMutualInfo_eq (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = H[X | Z ; μ] + H[Y | Z; μ] - H[⟨X, Y⟩ | Z ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 := (indepFun_const c).mutualInfo_eq_zero hX measurable_const\n\nlemma IndepFun.condEntropy_eq_entropy {μ : Measure Ω} (h : IndepFun X Y μ)\n    (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[X ; μ] := by\n  have := h.mutualInfo_eq_zero hX hY\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY] at this\n  linarith\n\n/-- $H[X, Y] = H[X] + H[Y]$ if and only if $X, Y$ are independent. -/\nlemma entropy_pair_eq_add (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y ; μ] ↔ IndepFun X Y μ := by\n  rw [eq_comm, ← sub_eq_zero, ← mutualInfo_eq_zero hX hY]; rfl\n\n/-- If $X, Y$ are independent, then $H[X, Y] = H[X] + H[Y]$. -/\nprotected alias ⟨_, IndepFun.entropy_pair_eq_add⟩ := entropy_pair_eq_add\n\n/-- The conditional mutual information $I[X : Y| Z]$ is the mutual information of $X| Z=z$ and\n$Y| Z=z$, integrated over $z$. -/\nnoncomputable\ndef condMutualInfo (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω := by volume_tac) :\n    ℝ := (μ.map Z)[fun z ↦ H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]]\n\nlemma condMutualInfo_def (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω) :\n    condMutualInfo X Y Z μ = (μ.map Z)[fun z ↦\n      H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]] := rfl\n\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \";\" μ \"]\" => condMutualInfo X Y Z μ\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \"]\" => condMutualInfo X Y Z volume\n\n/-- The conditional mutual information agrees with the information of the conditional kernel.\n-/\nlemma condMutualInfo_eq_kernel_mutualInfo\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = Ik[condDistrib (⟨X, Y⟩) Z μ, μ.map Z] := by\n  rcases finiteSupport_of_finiteRange (μ := μ) (X := Z) with ⟨A, hA⟩\n  simp_rw [condMutualInfo_def, entropy_def, kernel.mutualInfo, kernel.entropy,\n    integral_eq_sum' _ hA, smul_eq_mul, mul_sub, mul_add, Finset.sum_sub_distrib, Finset.sum_add_distrib]\n  congr with x\n  · have h := condDistrib_fst_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hX hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · have h := condDistrib_snd_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hY hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [condDistrib_apply (hX.prod_mk hY) hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n\nlemma condMutualInfo_eq_integral_mutualInfo :\n    I[X : Y | Z ; μ] = (μ.map Z)[fun z ↦ I[X : Y ; μ[| Z ⁻¹' {z}]]] := rfl\n\nlemma condMutualInfo_eq_sum [IsFiniteMeasure μ] (hZ : Measurable Z) [FiniteRange Z] :\n    I[X : Y | Z ; μ] = ∑ z in FiniteRange.toFinset Z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_integral_mutualInfo, integral_eq_sum' _ (FiniteRange.null_of_compl _ Z)]\n  congr 1 with z\n  rw [map_apply hZ (MeasurableSet.singleton z)]\n  rfl\n\n/-- A variant of `condMutualInfo_eq_sum` when `Z` has finite codomain. -/\nlemma condMutualInfo_eq_sum' [IsFiniteMeasure μ] (hZ : Measurable Z) [Fintype U] :\n    I[X : Y | Z ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_sum hZ]\n  apply Finset.sum_subset\n  . simp\n  intro z _ hz\n  have : Z ⁻¹' {z} = ∅ := by\n    ext ω\n    simp at hz\n    simp [hz]\n  simp [this]\n\n/-- $I[X : Y | Z] = I[Y : X | Z]$. -/\nlemma condMutualInfo_comm\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω) :\n    I[X : Y | Z ; μ] = I[Y : X | Z ; μ] := by\n  simp_rw [condMutualInfo_def, add_comm, entropy_comm hX hY]\n\n/-- Conditional information is non-nonegative. -/\nlemma condMutualInfo_nonneg\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y | Z ; μ] := by\n  refine integral_nonneg (fun z ↦ ?_)\n  exact mutualInfo_nonneg hX hY _\n\n/-- $$ I[X : Y| Z] = H[X| Z] + H[Y| Z] - H[X, Y| Z].$$ -/\nlemma condMutualInfo_eq (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = H[X | Z ; μ] + H[Y | Z; μ] - H[⟨X, Y⟩ | Z ; μ] := by\n  rw [condMutualInfo_eq_kernel_mutualInfo hX hY hZ, kernel.mutualInfo,\n    kernel.entropy_congr (condDistrib_fst_ae_eq hX hY hZ _),\n    kernel.entropy_congr (condDistrib_snd_ae_eq hX hY hZ _),\n    condEntropy_eq_kernel_entropy hX hZ, condEntropy_eq_kernel_entropy hY hZ,\n    condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ]\n\n","proof":":= by\n  rw [condMutualInfo_eq hX hY hZ, cond_chain_rule _ hX hY hZ]\n  ring","declId":"PFR.ForMathlib.Entropy.Basic.846_0.C2VYVgxGWlr8Qwd","decl":"/-- $$ I[X : Y| Z] = H[X| Z] - H[X|Y, Z].$$ -/\nlemma condMutualInfo_eq' (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = H[X | Z ; μ] - H[X | ⟨Y, Z⟩ ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 := (indepFun_const c).mutualInfo_eq_zero hX measurable_const\n\nlemma IndepFun.condEntropy_eq_entropy {μ : Measure Ω} (h : IndepFun X Y μ)\n    (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[X ; μ] := by\n  have := h.mutualInfo_eq_zero hX hY\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY] at this\n  linarith\n\n/-- $H[X, Y] = H[X] + H[Y]$ if and only if $X, Y$ are independent. -/\nlemma entropy_pair_eq_add (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y ; μ] ↔ IndepFun X Y μ := by\n  rw [eq_comm, ← sub_eq_zero, ← mutualInfo_eq_zero hX hY]; rfl\n\n/-- If $X, Y$ are independent, then $H[X, Y] = H[X] + H[Y]$. -/\nprotected alias ⟨_, IndepFun.entropy_pair_eq_add⟩ := entropy_pair_eq_add\n\n/-- The conditional mutual information $I[X : Y| Z]$ is the mutual information of $X| Z=z$ and\n$Y| Z=z$, integrated over $z$. -/\nnoncomputable\ndef condMutualInfo (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω := by volume_tac) :\n    ℝ := (μ.map Z)[fun z ↦ H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]]\n\nlemma condMutualInfo_def (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω) :\n    condMutualInfo X Y Z μ = (μ.map Z)[fun z ↦\n      H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]] := rfl\n\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \";\" μ \"]\" => condMutualInfo X Y Z μ\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \"]\" => condMutualInfo X Y Z volume\n\n/-- The conditional mutual information agrees with the information of the conditional kernel.\n-/\nlemma condMutualInfo_eq_kernel_mutualInfo\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = Ik[condDistrib (⟨X, Y⟩) Z μ, μ.map Z] := by\n  rcases finiteSupport_of_finiteRange (μ := μ) (X := Z) with ⟨A, hA⟩\n  simp_rw [condMutualInfo_def, entropy_def, kernel.mutualInfo, kernel.entropy,\n    integral_eq_sum' _ hA, smul_eq_mul, mul_sub, mul_add, Finset.sum_sub_distrib, Finset.sum_add_distrib]\n  congr with x\n  · have h := condDistrib_fst_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hX hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · have h := condDistrib_snd_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hY hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [condDistrib_apply (hX.prod_mk hY) hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n\nlemma condMutualInfo_eq_integral_mutualInfo :\n    I[X : Y | Z ; μ] = (μ.map Z)[fun z ↦ I[X : Y ; μ[| Z ⁻¹' {z}]]] := rfl\n\nlemma condMutualInfo_eq_sum [IsFiniteMeasure μ] (hZ : Measurable Z) [FiniteRange Z] :\n    I[X : Y | Z ; μ] = ∑ z in FiniteRange.toFinset Z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_integral_mutualInfo, integral_eq_sum' _ (FiniteRange.null_of_compl _ Z)]\n  congr 1 with z\n  rw [map_apply hZ (MeasurableSet.singleton z)]\n  rfl\n\n/-- A variant of `condMutualInfo_eq_sum` when `Z` has finite codomain. -/\nlemma condMutualInfo_eq_sum' [IsFiniteMeasure μ] (hZ : Measurable Z) [Fintype U] :\n    I[X : Y | Z ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_sum hZ]\n  apply Finset.sum_subset\n  . simp\n  intro z _ hz\n  have : Z ⁻¹' {z} = ∅ := by\n    ext ω\n    simp at hz\n    simp [hz]\n  simp [this]\n\n/-- $I[X : Y | Z] = I[Y : X | Z]$. -/\nlemma condMutualInfo_comm\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω) :\n    I[X : Y | Z ; μ] = I[Y : X | Z ; μ] := by\n  simp_rw [condMutualInfo_def, add_comm, entropy_comm hX hY]\n\n/-- Conditional information is non-nonegative. -/\nlemma condMutualInfo_nonneg\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y | Z ; μ] := by\n  refine integral_nonneg (fun z ↦ ?_)\n  exact mutualInfo_nonneg hX hY _\n\n/-- $$ I[X : Y| Z] = H[X| Z] + H[Y| Z] - H[X, Y| Z].$$ -/\nlemma condMutualInfo_eq (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = H[X | Z ; μ] + H[Y | Z; μ] - H[⟨X, Y⟩ | Z ; μ] := by\n  rw [condMutualInfo_eq_kernel_mutualInfo hX hY hZ, kernel.mutualInfo,\n    kernel.entropy_congr (condDistrib_fst_ae_eq hX hY hZ _),\n    kernel.entropy_congr (condDistrib_snd_ae_eq hX hY hZ _),\n    condEntropy_eq_kernel_entropy hX hZ, condEntropy_eq_kernel_entropy hY hZ,\n    condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ]\n\n/-- $$ I[X : Y| Z] = H[X| Z] - H[X|Y, Z].$$ -/\nlemma condMutualInfo_eq' (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = H[X | Z ; μ] - H[X | ⟨Y, Z⟩ ; μ] := by\n  rw [condMutualInfo_eq hX hY hZ, cond_chain_rule _ hX hY hZ]\n  ring\n\n","proof":":= by\n  have hM : Measurable (Function.uncurry f ∘ ⟨Z, X⟩) :=\n    (measurable_of_countable _).comp (hZ.prod_mk hX)\n  have hM : Measurable fun ω ↦ f (Z ω) (X ω) := hM\n  rw [condMutualInfo_eq hM hY hZ, condMutualInfo_eq hX hY hZ]\n  let g : U → (S × T) → (V × T) := fun z (x, y) ↦ (f z x, y)\n  have hg : ∀ t, Function.Injective (g t) :=\n    fun _ _ _ h ↦ Prod.ext_iff.2 ⟨hf _ (Prod.ext_iff.1 h).1, (Prod.ext_iff.1 h).2⟩\n  rw [← condEntropy_of_injective μ (hX.prod_mk hY) hZ g hg, ← condEntropy_of_injective μ hX hZ _ hf]","declId":"PFR.ForMathlib.Entropy.Basic.853_0.C2VYVgxGWlr8Qwd","decl":"/-- If $f(Z, X)$ is injective for each fixed $Z$, then $I[f(Z, X) : Y| Z] = I[X : Y| Z]$.-/\nlemma condMutualInfo_of_inj_map [IsProbabilityMeasure μ]\n  (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n  {V : Type*} [Nonempty V] [MeasurableSpace V] [MeasurableSingletonClass V] [Countable V]\n  (f : U → S → V) (hf : ∀ t, Function.Injective (f t)) [FiniteRange Z] :\n    I[fun ω ↦ f (Z ω) (X ω) : Y | Z ; μ] =\n    I[X : Y | Z ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 := (indepFun_const c).mutualInfo_eq_zero hX measurable_const\n\nlemma IndepFun.condEntropy_eq_entropy {μ : Measure Ω} (h : IndepFun X Y μ)\n    (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[X ; μ] := by\n  have := h.mutualInfo_eq_zero hX hY\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY] at this\n  linarith\n\n/-- $H[X, Y] = H[X] + H[Y]$ if and only if $X, Y$ are independent. -/\nlemma entropy_pair_eq_add (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y ; μ] ↔ IndepFun X Y μ := by\n  rw [eq_comm, ← sub_eq_zero, ← mutualInfo_eq_zero hX hY]; rfl\n\n/-- If $X, Y$ are independent, then $H[X, Y] = H[X] + H[Y]$. -/\nprotected alias ⟨_, IndepFun.entropy_pair_eq_add⟩ := entropy_pair_eq_add\n\n/-- The conditional mutual information $I[X : Y| Z]$ is the mutual information of $X| Z=z$ and\n$Y| Z=z$, integrated over $z$. -/\nnoncomputable\ndef condMutualInfo (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω := by volume_tac) :\n    ℝ := (μ.map Z)[fun z ↦ H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]]\n\nlemma condMutualInfo_def (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω) :\n    condMutualInfo X Y Z μ = (μ.map Z)[fun z ↦\n      H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]] := rfl\n\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \";\" μ \"]\" => condMutualInfo X Y Z μ\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \"]\" => condMutualInfo X Y Z volume\n\n/-- The conditional mutual information agrees with the information of the conditional kernel.\n-/\nlemma condMutualInfo_eq_kernel_mutualInfo\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = Ik[condDistrib (⟨X, Y⟩) Z μ, μ.map Z] := by\n  rcases finiteSupport_of_finiteRange (μ := μ) (X := Z) with ⟨A, hA⟩\n  simp_rw [condMutualInfo_def, entropy_def, kernel.mutualInfo, kernel.entropy,\n    integral_eq_sum' _ hA, smul_eq_mul, mul_sub, mul_add, Finset.sum_sub_distrib, Finset.sum_add_distrib]\n  congr with x\n  · have h := condDistrib_fst_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hX hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · have h := condDistrib_snd_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hY hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [condDistrib_apply (hX.prod_mk hY) hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n\nlemma condMutualInfo_eq_integral_mutualInfo :\n    I[X : Y | Z ; μ] = (μ.map Z)[fun z ↦ I[X : Y ; μ[| Z ⁻¹' {z}]]] := rfl\n\nlemma condMutualInfo_eq_sum [IsFiniteMeasure μ] (hZ : Measurable Z) [FiniteRange Z] :\n    I[X : Y | Z ; μ] = ∑ z in FiniteRange.toFinset Z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_integral_mutualInfo, integral_eq_sum' _ (FiniteRange.null_of_compl _ Z)]\n  congr 1 with z\n  rw [map_apply hZ (MeasurableSet.singleton z)]\n  rfl\n\n/-- A variant of `condMutualInfo_eq_sum` when `Z` has finite codomain. -/\nlemma condMutualInfo_eq_sum' [IsFiniteMeasure μ] (hZ : Measurable Z) [Fintype U] :\n    I[X : Y | Z ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_sum hZ]\n  apply Finset.sum_subset\n  . simp\n  intro z _ hz\n  have : Z ⁻¹' {z} = ∅ := by\n    ext ω\n    simp at hz\n    simp [hz]\n  simp [this]\n\n/-- $I[X : Y | Z] = I[Y : X | Z]$. -/\nlemma condMutualInfo_comm\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω) :\n    I[X : Y | Z ; μ] = I[Y : X | Z ; μ] := by\n  simp_rw [condMutualInfo_def, add_comm, entropy_comm hX hY]\n\n/-- Conditional information is non-nonegative. -/\nlemma condMutualInfo_nonneg\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y | Z ; μ] := by\n  refine integral_nonneg (fun z ↦ ?_)\n  exact mutualInfo_nonneg hX hY _\n\n/-- $$ I[X : Y| Z] = H[X| Z] + H[Y| Z] - H[X, Y| Z].$$ -/\nlemma condMutualInfo_eq (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = H[X | Z ; μ] + H[Y | Z; μ] - H[⟨X, Y⟩ | Z ; μ] := by\n  rw [condMutualInfo_eq_kernel_mutualInfo hX hY hZ, kernel.mutualInfo,\n    kernel.entropy_congr (condDistrib_fst_ae_eq hX hY hZ _),\n    kernel.entropy_congr (condDistrib_snd_ae_eq hX hY hZ _),\n    condEntropy_eq_kernel_entropy hX hZ, condEntropy_eq_kernel_entropy hY hZ,\n    condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ]\n\n/-- $$ I[X : Y| Z] = H[X| Z] - H[X|Y, Z].$$ -/\nlemma condMutualInfo_eq' (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = H[X | Z ; μ] - H[X | ⟨Y, Z⟩ ; μ] := by\n  rw [condMutualInfo_eq hX hY hZ, cond_chain_rule _ hX hY hZ]\n  ring\n\n/-- If $f(Z, X)$ is injective for each fixed $Z$, then $I[f(Z, X) : Y| Z] = I[X : Y| Z]$.-/\nlemma condMutualInfo_of_inj_map [IsProbabilityMeasure μ]\n  (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n  {V : Type*} [Nonempty V] [MeasurableSpace V] [MeasurableSingletonClass V] [Countable V]\n  (f : U → S → V) (hf : ∀ t, Function.Injective (f t)) [FiniteRange Z] :\n    I[fun ω ↦ f (Z ω) (X ω) : Y | Z ; μ] =\n    I[X : Y | Z ; μ] := by\n  have hM : Measurable (Function.uncurry f ∘ ⟨Z, X⟩) :=\n    (measurable_of_countable _).comp (hZ.prod_mk hX)\n  have hM : Measurable fun ω ↦ f (Z ω) (X ω) := hM\n  rw [condMutualInfo_eq hM hY hZ, condMutualInfo_eq hX hY hZ]\n  let g : U → (S × T) → (V × T) := fun z (x, y) ↦ (f z x, y)\n  have hg : ∀ t, Function.Injective (g t) :=\n    fun _ _ _ h ↦ Prod.ext_iff.2 ⟨hf _ (Prod.ext_iff.1 h).1, (Prod.ext_iff.1 h).2⟩\n  rw [← condEntropy_of_injective μ (hX.prod_mk hY) hZ g hg, ← condEntropy_of_injective μ hX hZ _ hf]\n\n","proof":":= by\n  rw [condEntropy_prod_eq_sum _ hY hZ]\n  have : H[X | Y ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ] := by\n    rw [← Finset.sum_mul, sum_measure_preimage_singleton' μ hZ, one_mul]\n  rw [this]\n  congr with w\n  rcases eq_or_ne (μ (Z ⁻¹' {w})) 0 with hw|hw\n  · simp [hw]\n  congr 1\n  have : IsProbabilityMeasure (μ[|Z ⁻¹' {w}]) := cond_isProbabilityMeasure μ hw\n  apply IdentDistrib.condEntropy_eq hX hY hX hY\n  exact (h.identDistrib_cond (MeasurableSet.singleton w) (hX.prod_mk hY) hZ hw).symm","declId":"PFR.ForMathlib.Entropy.Basic.869_0.C2VYVgxGWlr8Qwd","decl":"lemma condEntropy_prod_eq_of_indepFun [Fintype T] [Fintype U] [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X]\n    (h : IndepFun (⟨X, Y⟩) Z μ) :\n    H[X | ⟨Y, Z⟩ ; μ] = H[X | Y ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 := (indepFun_const c).mutualInfo_eq_zero hX measurable_const\n\nlemma IndepFun.condEntropy_eq_entropy {μ : Measure Ω} (h : IndepFun X Y μ)\n    (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[X ; μ] := by\n  have := h.mutualInfo_eq_zero hX hY\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY] at this\n  linarith\n\n/-- $H[X, Y] = H[X] + H[Y]$ if and only if $X, Y$ are independent. -/\nlemma entropy_pair_eq_add (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y ; μ] ↔ IndepFun X Y μ := by\n  rw [eq_comm, ← sub_eq_zero, ← mutualInfo_eq_zero hX hY]; rfl\n\n/-- If $X, Y$ are independent, then $H[X, Y] = H[X] + H[Y]$. -/\nprotected alias ⟨_, IndepFun.entropy_pair_eq_add⟩ := entropy_pair_eq_add\n\n/-- The conditional mutual information $I[X : Y| Z]$ is the mutual information of $X| Z=z$ and\n$Y| Z=z$, integrated over $z$. -/\nnoncomputable\ndef condMutualInfo (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω := by volume_tac) :\n    ℝ := (μ.map Z)[fun z ↦ H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]]\n\nlemma condMutualInfo_def (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω) :\n    condMutualInfo X Y Z μ = (μ.map Z)[fun z ↦\n      H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]] := rfl\n\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \";\" μ \"]\" => condMutualInfo X Y Z μ\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \"]\" => condMutualInfo X Y Z volume\n\n/-- The conditional mutual information agrees with the information of the conditional kernel.\n-/\nlemma condMutualInfo_eq_kernel_mutualInfo\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = Ik[condDistrib (⟨X, Y⟩) Z μ, μ.map Z] := by\n  rcases finiteSupport_of_finiteRange (μ := μ) (X := Z) with ⟨A, hA⟩\n  simp_rw [condMutualInfo_def, entropy_def, kernel.mutualInfo, kernel.entropy,\n    integral_eq_sum' _ hA, smul_eq_mul, mul_sub, mul_add, Finset.sum_sub_distrib, Finset.sum_add_distrib]\n  congr with x\n  · have h := condDistrib_fst_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hX hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · have h := condDistrib_snd_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hY hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [condDistrib_apply (hX.prod_mk hY) hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n\nlemma condMutualInfo_eq_integral_mutualInfo :\n    I[X : Y | Z ; μ] = (μ.map Z)[fun z ↦ I[X : Y ; μ[| Z ⁻¹' {z}]]] := rfl\n\nlemma condMutualInfo_eq_sum [IsFiniteMeasure μ] (hZ : Measurable Z) [FiniteRange Z] :\n    I[X : Y | Z ; μ] = ∑ z in FiniteRange.toFinset Z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_integral_mutualInfo, integral_eq_sum' _ (FiniteRange.null_of_compl _ Z)]\n  congr 1 with z\n  rw [map_apply hZ (MeasurableSet.singleton z)]\n  rfl\n\n/-- A variant of `condMutualInfo_eq_sum` when `Z` has finite codomain. -/\nlemma condMutualInfo_eq_sum' [IsFiniteMeasure μ] (hZ : Measurable Z) [Fintype U] :\n    I[X : Y | Z ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_sum hZ]\n  apply Finset.sum_subset\n  . simp\n  intro z _ hz\n  have : Z ⁻¹' {z} = ∅ := by\n    ext ω\n    simp at hz\n    simp [hz]\n  simp [this]\n\n/-- $I[X : Y | Z] = I[Y : X | Z]$. -/\nlemma condMutualInfo_comm\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω) :\n    I[X : Y | Z ; μ] = I[Y : X | Z ; μ] := by\n  simp_rw [condMutualInfo_def, add_comm, entropy_comm hX hY]\n\n/-- Conditional information is non-nonegative. -/\nlemma condMutualInfo_nonneg\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y | Z ; μ] := by\n  refine integral_nonneg (fun z ↦ ?_)\n  exact mutualInfo_nonneg hX hY _\n\n/-- $$ I[X : Y| Z] = H[X| Z] + H[Y| Z] - H[X, Y| Z].$$ -/\nlemma condMutualInfo_eq (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = H[X | Z ; μ] + H[Y | Z; μ] - H[⟨X, Y⟩ | Z ; μ] := by\n  rw [condMutualInfo_eq_kernel_mutualInfo hX hY hZ, kernel.mutualInfo,\n    kernel.entropy_congr (condDistrib_fst_ae_eq hX hY hZ _),\n    kernel.entropy_congr (condDistrib_snd_ae_eq hX hY hZ _),\n    condEntropy_eq_kernel_entropy hX hZ, condEntropy_eq_kernel_entropy hY hZ,\n    condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ]\n\n/-- $$ I[X : Y| Z] = H[X| Z] - H[X|Y, Z].$$ -/\nlemma condMutualInfo_eq' (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = H[X | Z ; μ] - H[X | ⟨Y, Z⟩ ; μ] := by\n  rw [condMutualInfo_eq hX hY hZ, cond_chain_rule _ hX hY hZ]\n  ring\n\n/-- If $f(Z, X)$ is injective for each fixed $Z$, then $I[f(Z, X) : Y| Z] = I[X : Y| Z]$.-/\nlemma condMutualInfo_of_inj_map [IsProbabilityMeasure μ]\n  (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n  {V : Type*} [Nonempty V] [MeasurableSpace V] [MeasurableSingletonClass V] [Countable V]\n  (f : U → S → V) (hf : ∀ t, Function.Injective (f t)) [FiniteRange Z] :\n    I[fun ω ↦ f (Z ω) (X ω) : Y | Z ; μ] =\n    I[X : Y | Z ; μ] := by\n  have hM : Measurable (Function.uncurry f ∘ ⟨Z, X⟩) :=\n    (measurable_of_countable _).comp (hZ.prod_mk hX)\n  have hM : Measurable fun ω ↦ f (Z ω) (X ω) := hM\n  rw [condMutualInfo_eq hM hY hZ, condMutualInfo_eq hX hY hZ]\n  let g : U → (S × T) → (V × T) := fun z (x, y) ↦ (f z x, y)\n  have hg : ∀ t, Function.Injective (g t) :=\n    fun _ _ _ h ↦ Prod.ext_iff.2 ⟨hf _ (Prod.ext_iff.1 h).1, (Prod.ext_iff.1 h).2⟩\n  rw [← condEntropy_of_injective μ (hX.prod_mk hY) hZ g hg, ← condEntropy_of_injective μ hX hZ _ hf]\n\nlemma condEntropy_prod_eq_of_indepFun [Fintype T] [Fintype U] [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X]\n    (h : IndepFun (⟨X, Y⟩) Z μ) :\n    H[X | ⟨Y, Z⟩ ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_prod_eq_sum _ hY hZ]\n  have : H[X | Y ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ] := by\n    rw [← Finset.sum_mul, sum_measure_preimage_singleton' μ hZ, one_mul]\n  rw [this]\n  congr with w\n  rcases eq_or_ne (μ (Z ⁻¹' {w})) 0 with hw|hw\n  · simp [hw]\n  congr 1\n  have : IsProbabilityMeasure (μ[|Z ⁻¹' {w}]) := cond_isProbabilityMeasure μ hw\n  apply IdentDistrib.condEntropy_eq hX hY hX hY\n  exact (h.identDistrib_cond (MeasurableSet.singleton w) (hX.prod_mk hY) hZ hw).symm\n\nsection IsProbabilityMeasure\nvariable (μ : Measure Ω) [IsProbabilityMeasure μ] [MeasurableSingletonClass S]\n  [MeasurableSingletonClass T]\n\n","proof":":= by\n  rw [mutualInfo_def, chain_rule _ hX hY, add_comm, add_sub_add_left_eq_sub]","declId":"PFR.ForMathlib.Entropy.Basic.889_0.C2VYVgxGWlr8Qwd","decl":"/-- $$ H[X] - H[X|Y] = I[X : Y] $$ -/\nlemma entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X ; μ] - H[X | Y ; μ] = I[X : Y ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 := (indepFun_const c).mutualInfo_eq_zero hX measurable_const\n\nlemma IndepFun.condEntropy_eq_entropy {μ : Measure Ω} (h : IndepFun X Y μ)\n    (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[X ; μ] := by\n  have := h.mutualInfo_eq_zero hX hY\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY] at this\n  linarith\n\n/-- $H[X, Y] = H[X] + H[Y]$ if and only if $X, Y$ are independent. -/\nlemma entropy_pair_eq_add (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y ; μ] ↔ IndepFun X Y μ := by\n  rw [eq_comm, ← sub_eq_zero, ← mutualInfo_eq_zero hX hY]; rfl\n\n/-- If $X, Y$ are independent, then $H[X, Y] = H[X] + H[Y]$. -/\nprotected alias ⟨_, IndepFun.entropy_pair_eq_add⟩ := entropy_pair_eq_add\n\n/-- The conditional mutual information $I[X : Y| Z]$ is the mutual information of $X| Z=z$ and\n$Y| Z=z$, integrated over $z$. -/\nnoncomputable\ndef condMutualInfo (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω := by volume_tac) :\n    ℝ := (μ.map Z)[fun z ↦ H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]]\n\nlemma condMutualInfo_def (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω) :\n    condMutualInfo X Y Z μ = (μ.map Z)[fun z ↦\n      H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]] := rfl\n\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \";\" μ \"]\" => condMutualInfo X Y Z μ\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \"]\" => condMutualInfo X Y Z volume\n\n/-- The conditional mutual information agrees with the information of the conditional kernel.\n-/\nlemma condMutualInfo_eq_kernel_mutualInfo\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = Ik[condDistrib (⟨X, Y⟩) Z μ, μ.map Z] := by\n  rcases finiteSupport_of_finiteRange (μ := μ) (X := Z) with ⟨A, hA⟩\n  simp_rw [condMutualInfo_def, entropy_def, kernel.mutualInfo, kernel.entropy,\n    integral_eq_sum' _ hA, smul_eq_mul, mul_sub, mul_add, Finset.sum_sub_distrib, Finset.sum_add_distrib]\n  congr with x\n  · have h := condDistrib_fst_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hX hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · have h := condDistrib_snd_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hY hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [condDistrib_apply (hX.prod_mk hY) hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n\nlemma condMutualInfo_eq_integral_mutualInfo :\n    I[X : Y | Z ; μ] = (μ.map Z)[fun z ↦ I[X : Y ; μ[| Z ⁻¹' {z}]]] := rfl\n\nlemma condMutualInfo_eq_sum [IsFiniteMeasure μ] (hZ : Measurable Z) [FiniteRange Z] :\n    I[X : Y | Z ; μ] = ∑ z in FiniteRange.toFinset Z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_integral_mutualInfo, integral_eq_sum' _ (FiniteRange.null_of_compl _ Z)]\n  congr 1 with z\n  rw [map_apply hZ (MeasurableSet.singleton z)]\n  rfl\n\n/-- A variant of `condMutualInfo_eq_sum` when `Z` has finite codomain. -/\nlemma condMutualInfo_eq_sum' [IsFiniteMeasure μ] (hZ : Measurable Z) [Fintype U] :\n    I[X : Y | Z ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_sum hZ]\n  apply Finset.sum_subset\n  . simp\n  intro z _ hz\n  have : Z ⁻¹' {z} = ∅ := by\n    ext ω\n    simp at hz\n    simp [hz]\n  simp [this]\n\n/-- $I[X : Y | Z] = I[Y : X | Z]$. -/\nlemma condMutualInfo_comm\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω) :\n    I[X : Y | Z ; μ] = I[Y : X | Z ; μ] := by\n  simp_rw [condMutualInfo_def, add_comm, entropy_comm hX hY]\n\n/-- Conditional information is non-nonegative. -/\nlemma condMutualInfo_nonneg\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y | Z ; μ] := by\n  refine integral_nonneg (fun z ↦ ?_)\n  exact mutualInfo_nonneg hX hY _\n\n/-- $$ I[X : Y| Z] = H[X| Z] + H[Y| Z] - H[X, Y| Z].$$ -/\nlemma condMutualInfo_eq (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = H[X | Z ; μ] + H[Y | Z; μ] - H[⟨X, Y⟩ | Z ; μ] := by\n  rw [condMutualInfo_eq_kernel_mutualInfo hX hY hZ, kernel.mutualInfo,\n    kernel.entropy_congr (condDistrib_fst_ae_eq hX hY hZ _),\n    kernel.entropy_congr (condDistrib_snd_ae_eq hX hY hZ _),\n    condEntropy_eq_kernel_entropy hX hZ, condEntropy_eq_kernel_entropy hY hZ,\n    condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ]\n\n/-- $$ I[X : Y| Z] = H[X| Z] - H[X|Y, Z].$$ -/\nlemma condMutualInfo_eq' (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = H[X | Z ; μ] - H[X | ⟨Y, Z⟩ ; μ] := by\n  rw [condMutualInfo_eq hX hY hZ, cond_chain_rule _ hX hY hZ]\n  ring\n\n/-- If $f(Z, X)$ is injective for each fixed $Z$, then $I[f(Z, X) : Y| Z] = I[X : Y| Z]$.-/\nlemma condMutualInfo_of_inj_map [IsProbabilityMeasure μ]\n  (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n  {V : Type*} [Nonempty V] [MeasurableSpace V] [MeasurableSingletonClass V] [Countable V]\n  (f : U → S → V) (hf : ∀ t, Function.Injective (f t)) [FiniteRange Z] :\n    I[fun ω ↦ f (Z ω) (X ω) : Y | Z ; μ] =\n    I[X : Y | Z ; μ] := by\n  have hM : Measurable (Function.uncurry f ∘ ⟨Z, X⟩) :=\n    (measurable_of_countable _).comp (hZ.prod_mk hX)\n  have hM : Measurable fun ω ↦ f (Z ω) (X ω) := hM\n  rw [condMutualInfo_eq hM hY hZ, condMutualInfo_eq hX hY hZ]\n  let g : U → (S × T) → (V × T) := fun z (x, y) ↦ (f z x, y)\n  have hg : ∀ t, Function.Injective (g t) :=\n    fun _ _ _ h ↦ Prod.ext_iff.2 ⟨hf _ (Prod.ext_iff.1 h).1, (Prod.ext_iff.1 h).2⟩\n  rw [← condEntropy_of_injective μ (hX.prod_mk hY) hZ g hg, ← condEntropy_of_injective μ hX hZ _ hf]\n\nlemma condEntropy_prod_eq_of_indepFun [Fintype T] [Fintype U] [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X]\n    (h : IndepFun (⟨X, Y⟩) Z μ) :\n    H[X | ⟨Y, Z⟩ ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_prod_eq_sum _ hY hZ]\n  have : H[X | Y ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ] := by\n    rw [← Finset.sum_mul, sum_measure_preimage_singleton' μ hZ, one_mul]\n  rw [this]\n  congr with w\n  rcases eq_or_ne (μ (Z ⁻¹' {w})) 0 with hw|hw\n  · simp [hw]\n  congr 1\n  have : IsProbabilityMeasure (μ[|Z ⁻¹' {w}]) := cond_isProbabilityMeasure μ hw\n  apply IdentDistrib.condEntropy_eq hX hY hX hY\n  exact (h.identDistrib_cond (MeasurableSet.singleton w) (hX.prod_mk hY) hZ hw).symm\n\nsection IsProbabilityMeasure\nvariable (μ : Measure Ω) [IsProbabilityMeasure μ] [MeasurableSingletonClass S]\n  [MeasurableSingletonClass T]\n\n/-- $$ H[X] - H[X|Y] = I[X : Y] $$ -/\nlemma entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X ; μ] - H[X | Y ; μ] = I[X : Y ; μ] := by\n  rw [mutualInfo_def, chain_rule _ hX hY, add_comm, add_sub_add_left_eq_sub]\n\n","proof":":=\n  sub_nonneg.1 $ by rw [entropy_sub_condEntropy _ hX hY]; exact mutualInfo_nonneg hX hY _","declId":"PFR.ForMathlib.Entropy.Basic.894_0.C2VYVgxGWlr8Qwd","decl":"/-- $$ H[X|Y] ≤ H[X] $$ -/\nlemma condEntropy_le_entropy (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] ≤ H[X ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 := (indepFun_const c).mutualInfo_eq_zero hX measurable_const\n\nlemma IndepFun.condEntropy_eq_entropy {μ : Measure Ω} (h : IndepFun X Y μ)\n    (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[X ; μ] := by\n  have := h.mutualInfo_eq_zero hX hY\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY] at this\n  linarith\n\n/-- $H[X, Y] = H[X] + H[Y]$ if and only if $X, Y$ are independent. -/\nlemma entropy_pair_eq_add (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y ; μ] ↔ IndepFun X Y μ := by\n  rw [eq_comm, ← sub_eq_zero, ← mutualInfo_eq_zero hX hY]; rfl\n\n/-- If $X, Y$ are independent, then $H[X, Y] = H[X] + H[Y]$. -/\nprotected alias ⟨_, IndepFun.entropy_pair_eq_add⟩ := entropy_pair_eq_add\n\n/-- The conditional mutual information $I[X : Y| Z]$ is the mutual information of $X| Z=z$ and\n$Y| Z=z$, integrated over $z$. -/\nnoncomputable\ndef condMutualInfo (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω := by volume_tac) :\n    ℝ := (μ.map Z)[fun z ↦ H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]]\n\nlemma condMutualInfo_def (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω) :\n    condMutualInfo X Y Z μ = (μ.map Z)[fun z ↦\n      H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]] := rfl\n\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \";\" μ \"]\" => condMutualInfo X Y Z μ\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \"]\" => condMutualInfo X Y Z volume\n\n/-- The conditional mutual information agrees with the information of the conditional kernel.\n-/\nlemma condMutualInfo_eq_kernel_mutualInfo\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = Ik[condDistrib (⟨X, Y⟩) Z μ, μ.map Z] := by\n  rcases finiteSupport_of_finiteRange (μ := μ) (X := Z) with ⟨A, hA⟩\n  simp_rw [condMutualInfo_def, entropy_def, kernel.mutualInfo, kernel.entropy,\n    integral_eq_sum' _ hA, smul_eq_mul, mul_sub, mul_add, Finset.sum_sub_distrib, Finset.sum_add_distrib]\n  congr with x\n  · have h := condDistrib_fst_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hX hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · have h := condDistrib_snd_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hY hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [condDistrib_apply (hX.prod_mk hY) hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n\nlemma condMutualInfo_eq_integral_mutualInfo :\n    I[X : Y | Z ; μ] = (μ.map Z)[fun z ↦ I[X : Y ; μ[| Z ⁻¹' {z}]]] := rfl\n\nlemma condMutualInfo_eq_sum [IsFiniteMeasure μ] (hZ : Measurable Z) [FiniteRange Z] :\n    I[X : Y | Z ; μ] = ∑ z in FiniteRange.toFinset Z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_integral_mutualInfo, integral_eq_sum' _ (FiniteRange.null_of_compl _ Z)]\n  congr 1 with z\n  rw [map_apply hZ (MeasurableSet.singleton z)]\n  rfl\n\n/-- A variant of `condMutualInfo_eq_sum` when `Z` has finite codomain. -/\nlemma condMutualInfo_eq_sum' [IsFiniteMeasure μ] (hZ : Measurable Z) [Fintype U] :\n    I[X : Y | Z ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_sum hZ]\n  apply Finset.sum_subset\n  . simp\n  intro z _ hz\n  have : Z ⁻¹' {z} = ∅ := by\n    ext ω\n    simp at hz\n    simp [hz]\n  simp [this]\n\n/-- $I[X : Y | Z] = I[Y : X | Z]$. -/\nlemma condMutualInfo_comm\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω) :\n    I[X : Y | Z ; μ] = I[Y : X | Z ; μ] := by\n  simp_rw [condMutualInfo_def, add_comm, entropy_comm hX hY]\n\n/-- Conditional information is non-nonegative. -/\nlemma condMutualInfo_nonneg\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y | Z ; μ] := by\n  refine integral_nonneg (fun z ↦ ?_)\n  exact mutualInfo_nonneg hX hY _\n\n/-- $$ I[X : Y| Z] = H[X| Z] + H[Y| Z] - H[X, Y| Z].$$ -/\nlemma condMutualInfo_eq (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = H[X | Z ; μ] + H[Y | Z; μ] - H[⟨X, Y⟩ | Z ; μ] := by\n  rw [condMutualInfo_eq_kernel_mutualInfo hX hY hZ, kernel.mutualInfo,\n    kernel.entropy_congr (condDistrib_fst_ae_eq hX hY hZ _),\n    kernel.entropy_congr (condDistrib_snd_ae_eq hX hY hZ _),\n    condEntropy_eq_kernel_entropy hX hZ, condEntropy_eq_kernel_entropy hY hZ,\n    condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ]\n\n/-- $$ I[X : Y| Z] = H[X| Z] - H[X|Y, Z].$$ -/\nlemma condMutualInfo_eq' (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = H[X | Z ; μ] - H[X | ⟨Y, Z⟩ ; μ] := by\n  rw [condMutualInfo_eq hX hY hZ, cond_chain_rule _ hX hY hZ]\n  ring\n\n/-- If $f(Z, X)$ is injective for each fixed $Z$, then $I[f(Z, X) : Y| Z] = I[X : Y| Z]$.-/\nlemma condMutualInfo_of_inj_map [IsProbabilityMeasure μ]\n  (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n  {V : Type*} [Nonempty V] [MeasurableSpace V] [MeasurableSingletonClass V] [Countable V]\n  (f : U → S → V) (hf : ∀ t, Function.Injective (f t)) [FiniteRange Z] :\n    I[fun ω ↦ f (Z ω) (X ω) : Y | Z ; μ] =\n    I[X : Y | Z ; μ] := by\n  have hM : Measurable (Function.uncurry f ∘ ⟨Z, X⟩) :=\n    (measurable_of_countable _).comp (hZ.prod_mk hX)\n  have hM : Measurable fun ω ↦ f (Z ω) (X ω) := hM\n  rw [condMutualInfo_eq hM hY hZ, condMutualInfo_eq hX hY hZ]\n  let g : U → (S × T) → (V × T) := fun z (x, y) ↦ (f z x, y)\n  have hg : ∀ t, Function.Injective (g t) :=\n    fun _ _ _ h ↦ Prod.ext_iff.2 ⟨hf _ (Prod.ext_iff.1 h).1, (Prod.ext_iff.1 h).2⟩\n  rw [← condEntropy_of_injective μ (hX.prod_mk hY) hZ g hg, ← condEntropy_of_injective μ hX hZ _ hf]\n\nlemma condEntropy_prod_eq_of_indepFun [Fintype T] [Fintype U] [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X]\n    (h : IndepFun (⟨X, Y⟩) Z μ) :\n    H[X | ⟨Y, Z⟩ ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_prod_eq_sum _ hY hZ]\n  have : H[X | Y ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ] := by\n    rw [← Finset.sum_mul, sum_measure_preimage_singleton' μ hZ, one_mul]\n  rw [this]\n  congr with w\n  rcases eq_or_ne (μ (Z ⁻¹' {w})) 0 with hw|hw\n  · simp [hw]\n  congr 1\n  have : IsProbabilityMeasure (μ[|Z ⁻¹' {w}]) := cond_isProbabilityMeasure μ hw\n  apply IdentDistrib.condEntropy_eq hX hY hX hY\n  exact (h.identDistrib_cond (MeasurableSet.singleton w) (hX.prod_mk hY) hZ hw).symm\n\nsection IsProbabilityMeasure\nvariable (μ : Measure Ω) [IsProbabilityMeasure μ] [MeasurableSingletonClass S]\n  [MeasurableSingletonClass T]\n\n/-- $$ H[X] - H[X|Y] = I[X : Y] $$ -/\nlemma entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X ; μ] - H[X | Y ; μ] = I[X : Y ; μ] := by\n  rw [mutualInfo_def, chain_rule _ hX hY, add_comm, add_sub_add_left_eq_sub]\n\n/-- $$ H[X|Y] ≤ H[X] $$ -/\nlemma condEntropy_le_entropy (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] ≤ H[X ; μ] :=\n  sub_nonneg.1 $ by rw [entropy_sub_condEntropy _ hX hY]; exact mutualInfo_nonneg hX hY _\n\n","proof":":= by\n  rw [condEntropy_eq_kernel_entropy hX hZ, condEntropy_two_eq_kernel_entropy hX hY hZ]\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  refine (kernel.entropy_condKernel_le_entropy_snd ?_).trans_eq ?_\n  . apply kernel.aefiniteKernelSupport_condDistrib\n    all_goals measurability\n  exact kernel.entropy_congr (condDistrib_snd_ae_eq hY hX hZ _)","declId":"PFR.ForMathlib.Entropy.Basic.899_0.C2VYVgxGWlr8Qwd","decl":"/-- $H[X | Y, Z] \\leq H[X | Z]$ -/\nlemma entropy_submodular (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] ≤ H[X | Z ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 := (indepFun_const c).mutualInfo_eq_zero hX measurable_const\n\nlemma IndepFun.condEntropy_eq_entropy {μ : Measure Ω} (h : IndepFun X Y μ)\n    (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[X ; μ] := by\n  have := h.mutualInfo_eq_zero hX hY\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY] at this\n  linarith\n\n/-- $H[X, Y] = H[X] + H[Y]$ if and only if $X, Y$ are independent. -/\nlemma entropy_pair_eq_add (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y ; μ] ↔ IndepFun X Y μ := by\n  rw [eq_comm, ← sub_eq_zero, ← mutualInfo_eq_zero hX hY]; rfl\n\n/-- If $X, Y$ are independent, then $H[X, Y] = H[X] + H[Y]$. -/\nprotected alias ⟨_, IndepFun.entropy_pair_eq_add⟩ := entropy_pair_eq_add\n\n/-- The conditional mutual information $I[X : Y| Z]$ is the mutual information of $X| Z=z$ and\n$Y| Z=z$, integrated over $z$. -/\nnoncomputable\ndef condMutualInfo (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω := by volume_tac) :\n    ℝ := (μ.map Z)[fun z ↦ H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]]\n\nlemma condMutualInfo_def (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω) :\n    condMutualInfo X Y Z μ = (μ.map Z)[fun z ↦\n      H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]] := rfl\n\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \";\" μ \"]\" => condMutualInfo X Y Z μ\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \"]\" => condMutualInfo X Y Z volume\n\n/-- The conditional mutual information agrees with the information of the conditional kernel.\n-/\nlemma condMutualInfo_eq_kernel_mutualInfo\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = Ik[condDistrib (⟨X, Y⟩) Z μ, μ.map Z] := by\n  rcases finiteSupport_of_finiteRange (μ := μ) (X := Z) with ⟨A, hA⟩\n  simp_rw [condMutualInfo_def, entropy_def, kernel.mutualInfo, kernel.entropy,\n    integral_eq_sum' _ hA, smul_eq_mul, mul_sub, mul_add, Finset.sum_sub_distrib, Finset.sum_add_distrib]\n  congr with x\n  · have h := condDistrib_fst_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hX hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · have h := condDistrib_snd_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hY hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [condDistrib_apply (hX.prod_mk hY) hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n\nlemma condMutualInfo_eq_integral_mutualInfo :\n    I[X : Y | Z ; μ] = (μ.map Z)[fun z ↦ I[X : Y ; μ[| Z ⁻¹' {z}]]] := rfl\n\nlemma condMutualInfo_eq_sum [IsFiniteMeasure μ] (hZ : Measurable Z) [FiniteRange Z] :\n    I[X : Y | Z ; μ] = ∑ z in FiniteRange.toFinset Z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_integral_mutualInfo, integral_eq_sum' _ (FiniteRange.null_of_compl _ Z)]\n  congr 1 with z\n  rw [map_apply hZ (MeasurableSet.singleton z)]\n  rfl\n\n/-- A variant of `condMutualInfo_eq_sum` when `Z` has finite codomain. -/\nlemma condMutualInfo_eq_sum' [IsFiniteMeasure μ] (hZ : Measurable Z) [Fintype U] :\n    I[X : Y | Z ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_sum hZ]\n  apply Finset.sum_subset\n  . simp\n  intro z _ hz\n  have : Z ⁻¹' {z} = ∅ := by\n    ext ω\n    simp at hz\n    simp [hz]\n  simp [this]\n\n/-- $I[X : Y | Z] = I[Y : X | Z]$. -/\nlemma condMutualInfo_comm\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω) :\n    I[X : Y | Z ; μ] = I[Y : X | Z ; μ] := by\n  simp_rw [condMutualInfo_def, add_comm, entropy_comm hX hY]\n\n/-- Conditional information is non-nonegative. -/\nlemma condMutualInfo_nonneg\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y | Z ; μ] := by\n  refine integral_nonneg (fun z ↦ ?_)\n  exact mutualInfo_nonneg hX hY _\n\n/-- $$ I[X : Y| Z] = H[X| Z] + H[Y| Z] - H[X, Y| Z].$$ -/\nlemma condMutualInfo_eq (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = H[X | Z ; μ] + H[Y | Z; μ] - H[⟨X, Y⟩ | Z ; μ] := by\n  rw [condMutualInfo_eq_kernel_mutualInfo hX hY hZ, kernel.mutualInfo,\n    kernel.entropy_congr (condDistrib_fst_ae_eq hX hY hZ _),\n    kernel.entropy_congr (condDistrib_snd_ae_eq hX hY hZ _),\n    condEntropy_eq_kernel_entropy hX hZ, condEntropy_eq_kernel_entropy hY hZ,\n    condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ]\n\n/-- $$ I[X : Y| Z] = H[X| Z] - H[X|Y, Z].$$ -/\nlemma condMutualInfo_eq' (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = H[X | Z ; μ] - H[X | ⟨Y, Z⟩ ; μ] := by\n  rw [condMutualInfo_eq hX hY hZ, cond_chain_rule _ hX hY hZ]\n  ring\n\n/-- If $f(Z, X)$ is injective for each fixed $Z$, then $I[f(Z, X) : Y| Z] = I[X : Y| Z]$.-/\nlemma condMutualInfo_of_inj_map [IsProbabilityMeasure μ]\n  (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n  {V : Type*} [Nonempty V] [MeasurableSpace V] [MeasurableSingletonClass V] [Countable V]\n  (f : U → S → V) (hf : ∀ t, Function.Injective (f t)) [FiniteRange Z] :\n    I[fun ω ↦ f (Z ω) (X ω) : Y | Z ; μ] =\n    I[X : Y | Z ; μ] := by\n  have hM : Measurable (Function.uncurry f ∘ ⟨Z, X⟩) :=\n    (measurable_of_countable _).comp (hZ.prod_mk hX)\n  have hM : Measurable fun ω ↦ f (Z ω) (X ω) := hM\n  rw [condMutualInfo_eq hM hY hZ, condMutualInfo_eq hX hY hZ]\n  let g : U → (S × T) → (V × T) := fun z (x, y) ↦ (f z x, y)\n  have hg : ∀ t, Function.Injective (g t) :=\n    fun _ _ _ h ↦ Prod.ext_iff.2 ⟨hf _ (Prod.ext_iff.1 h).1, (Prod.ext_iff.1 h).2⟩\n  rw [← condEntropy_of_injective μ (hX.prod_mk hY) hZ g hg, ← condEntropy_of_injective μ hX hZ _ hf]\n\nlemma condEntropy_prod_eq_of_indepFun [Fintype T] [Fintype U] [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X]\n    (h : IndepFun (⟨X, Y⟩) Z μ) :\n    H[X | ⟨Y, Z⟩ ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_prod_eq_sum _ hY hZ]\n  have : H[X | Y ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ] := by\n    rw [← Finset.sum_mul, sum_measure_preimage_singleton' μ hZ, one_mul]\n  rw [this]\n  congr with w\n  rcases eq_or_ne (μ (Z ⁻¹' {w})) 0 with hw|hw\n  · simp [hw]\n  congr 1\n  have : IsProbabilityMeasure (μ[|Z ⁻¹' {w}]) := cond_isProbabilityMeasure μ hw\n  apply IdentDistrib.condEntropy_eq hX hY hX hY\n  exact (h.identDistrib_cond (MeasurableSet.singleton w) (hX.prod_mk hY) hZ hw).symm\n\nsection IsProbabilityMeasure\nvariable (μ : Measure Ω) [IsProbabilityMeasure μ] [MeasurableSingletonClass S]\n  [MeasurableSingletonClass T]\n\n/-- $$ H[X] - H[X|Y] = I[X : Y] $$ -/\nlemma entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X ; μ] - H[X | Y ; μ] = I[X : Y ; μ] := by\n  rw [mutualInfo_def, chain_rule _ hX hY, add_comm, add_sub_add_left_eq_sub]\n\n/-- $$ H[X|Y] ≤ H[X] $$ -/\nlemma condEntropy_le_entropy (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] ≤ H[X ; μ] :=\n  sub_nonneg.1 $ by rw [entropy_sub_condEntropy _ hX hY]; exact mutualInfo_nonneg hX hY _\n\n/-- $H[X | Y, Z] \\leq H[X | Z]$ -/\nlemma entropy_submodular (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] ≤ H[X | Z ; μ] := by\n  rw [condEntropy_eq_kernel_entropy hX hZ, condEntropy_two_eq_kernel_entropy hX hY hZ]\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  refine (kernel.entropy_condKernel_le_entropy_snd ?_).trans_eq ?_\n  . apply kernel.aefiniteKernelSupport_condDistrib\n    all_goals measurability\n  exact kernel.entropy_congr (condDistrib_snd_ae_eq hY hX hZ _)\n\n","proof":":= by\n  have h_joint : H[⟨Y, ⟨X, f ∘ X⟩⟩ ; μ] = H[⟨Y, X⟩ ; μ] := by\n    let g : T × S → T × S × U := fun (y, x) ↦ (y, (x, f x))\n    show H[g ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n    refine entropy_comp_of_injective μ (by exact Measurable.prod hY hX) g (fun _ _ h => ?_)\n    repeat rewrite [Prod.mk.injEq] at h\n    exact Prod.ext h.1 h.2.1\n  have hZ : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  rewrite [chain_rule'' μ hY hX, ← entropy_prod_comp hX μ f, ← h_joint,\n    ← chain_rule'' μ hY (Measurable.prod (by exact hX) (by exact hZ))]\n  exact entropy_submodular μ hY hX hZ","declId":"PFR.ForMathlib.Entropy.Basic.909_0.C2VYVgxGWlr8Qwd","decl":"/-- Data-processing inequality for the conditional entropy:\n$$ H[Y|f(X)] \\geq H[Y|X]$$\nTo upgrade this to equality, see `condEntropy_of_injective'` -/\nlemma condEntropy_comp_ge [FiniteRange X] [FiniteRange Y] (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (f : S → U) : H[Y | f ∘ X ; μ] ≥ H[Y | X; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 := (indepFun_const c).mutualInfo_eq_zero hX measurable_const\n\nlemma IndepFun.condEntropy_eq_entropy {μ : Measure Ω} (h : IndepFun X Y μ)\n    (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[X ; μ] := by\n  have := h.mutualInfo_eq_zero hX hY\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY] at this\n  linarith\n\n/-- $H[X, Y] = H[X] + H[Y]$ if and only if $X, Y$ are independent. -/\nlemma entropy_pair_eq_add (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y ; μ] ↔ IndepFun X Y μ := by\n  rw [eq_comm, ← sub_eq_zero, ← mutualInfo_eq_zero hX hY]; rfl\n\n/-- If $X, Y$ are independent, then $H[X, Y] = H[X] + H[Y]$. -/\nprotected alias ⟨_, IndepFun.entropy_pair_eq_add⟩ := entropy_pair_eq_add\n\n/-- The conditional mutual information $I[X : Y| Z]$ is the mutual information of $X| Z=z$ and\n$Y| Z=z$, integrated over $z$. -/\nnoncomputable\ndef condMutualInfo (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω := by volume_tac) :\n    ℝ := (μ.map Z)[fun z ↦ H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]]\n\nlemma condMutualInfo_def (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω) :\n    condMutualInfo X Y Z μ = (μ.map Z)[fun z ↦\n      H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]] := rfl\n\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \";\" μ \"]\" => condMutualInfo X Y Z μ\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \"]\" => condMutualInfo X Y Z volume\n\n/-- The conditional mutual information agrees with the information of the conditional kernel.\n-/\nlemma condMutualInfo_eq_kernel_mutualInfo\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = Ik[condDistrib (⟨X, Y⟩) Z μ, μ.map Z] := by\n  rcases finiteSupport_of_finiteRange (μ := μ) (X := Z) with ⟨A, hA⟩\n  simp_rw [condMutualInfo_def, entropy_def, kernel.mutualInfo, kernel.entropy,\n    integral_eq_sum' _ hA, smul_eq_mul, mul_sub, mul_add, Finset.sum_sub_distrib, Finset.sum_add_distrib]\n  congr with x\n  · have h := condDistrib_fst_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hX hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · have h := condDistrib_snd_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hY hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [condDistrib_apply (hX.prod_mk hY) hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n\nlemma condMutualInfo_eq_integral_mutualInfo :\n    I[X : Y | Z ; μ] = (μ.map Z)[fun z ↦ I[X : Y ; μ[| Z ⁻¹' {z}]]] := rfl\n\nlemma condMutualInfo_eq_sum [IsFiniteMeasure μ] (hZ : Measurable Z) [FiniteRange Z] :\n    I[X : Y | Z ; μ] = ∑ z in FiniteRange.toFinset Z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_integral_mutualInfo, integral_eq_sum' _ (FiniteRange.null_of_compl _ Z)]\n  congr 1 with z\n  rw [map_apply hZ (MeasurableSet.singleton z)]\n  rfl\n\n/-- A variant of `condMutualInfo_eq_sum` when `Z` has finite codomain. -/\nlemma condMutualInfo_eq_sum' [IsFiniteMeasure μ] (hZ : Measurable Z) [Fintype U] :\n    I[X : Y | Z ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_sum hZ]\n  apply Finset.sum_subset\n  . simp\n  intro z _ hz\n  have : Z ⁻¹' {z} = ∅ := by\n    ext ω\n    simp at hz\n    simp [hz]\n  simp [this]\n\n/-- $I[X : Y | Z] = I[Y : X | Z]$. -/\nlemma condMutualInfo_comm\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω) :\n    I[X : Y | Z ; μ] = I[Y : X | Z ; μ] := by\n  simp_rw [condMutualInfo_def, add_comm, entropy_comm hX hY]\n\n/-- Conditional information is non-nonegative. -/\nlemma condMutualInfo_nonneg\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y | Z ; μ] := by\n  refine integral_nonneg (fun z ↦ ?_)\n  exact mutualInfo_nonneg hX hY _\n\n/-- $$ I[X : Y| Z] = H[X| Z] + H[Y| Z] - H[X, Y| Z].$$ -/\nlemma condMutualInfo_eq (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = H[X | Z ; μ] + H[Y | Z; μ] - H[⟨X, Y⟩ | Z ; μ] := by\n  rw [condMutualInfo_eq_kernel_mutualInfo hX hY hZ, kernel.mutualInfo,\n    kernel.entropy_congr (condDistrib_fst_ae_eq hX hY hZ _),\n    kernel.entropy_congr (condDistrib_snd_ae_eq hX hY hZ _),\n    condEntropy_eq_kernel_entropy hX hZ, condEntropy_eq_kernel_entropy hY hZ,\n    condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ]\n\n/-- $$ I[X : Y| Z] = H[X| Z] - H[X|Y, Z].$$ -/\nlemma condMutualInfo_eq' (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = H[X | Z ; μ] - H[X | ⟨Y, Z⟩ ; μ] := by\n  rw [condMutualInfo_eq hX hY hZ, cond_chain_rule _ hX hY hZ]\n  ring\n\n/-- If $f(Z, X)$ is injective for each fixed $Z$, then $I[f(Z, X) : Y| Z] = I[X : Y| Z]$.-/\nlemma condMutualInfo_of_inj_map [IsProbabilityMeasure μ]\n  (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n  {V : Type*} [Nonempty V] [MeasurableSpace V] [MeasurableSingletonClass V] [Countable V]\n  (f : U → S → V) (hf : ∀ t, Function.Injective (f t)) [FiniteRange Z] :\n    I[fun ω ↦ f (Z ω) (X ω) : Y | Z ; μ] =\n    I[X : Y | Z ; μ] := by\n  have hM : Measurable (Function.uncurry f ∘ ⟨Z, X⟩) :=\n    (measurable_of_countable _).comp (hZ.prod_mk hX)\n  have hM : Measurable fun ω ↦ f (Z ω) (X ω) := hM\n  rw [condMutualInfo_eq hM hY hZ, condMutualInfo_eq hX hY hZ]\n  let g : U → (S × T) → (V × T) := fun z (x, y) ↦ (f z x, y)\n  have hg : ∀ t, Function.Injective (g t) :=\n    fun _ _ _ h ↦ Prod.ext_iff.2 ⟨hf _ (Prod.ext_iff.1 h).1, (Prod.ext_iff.1 h).2⟩\n  rw [← condEntropy_of_injective μ (hX.prod_mk hY) hZ g hg, ← condEntropy_of_injective μ hX hZ _ hf]\n\nlemma condEntropy_prod_eq_of_indepFun [Fintype T] [Fintype U] [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X]\n    (h : IndepFun (⟨X, Y⟩) Z μ) :\n    H[X | ⟨Y, Z⟩ ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_prod_eq_sum _ hY hZ]\n  have : H[X | Y ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ] := by\n    rw [← Finset.sum_mul, sum_measure_preimage_singleton' μ hZ, one_mul]\n  rw [this]\n  congr with w\n  rcases eq_or_ne (μ (Z ⁻¹' {w})) 0 with hw|hw\n  · simp [hw]\n  congr 1\n  have : IsProbabilityMeasure (μ[|Z ⁻¹' {w}]) := cond_isProbabilityMeasure μ hw\n  apply IdentDistrib.condEntropy_eq hX hY hX hY\n  exact (h.identDistrib_cond (MeasurableSet.singleton w) (hX.prod_mk hY) hZ hw).symm\n\nsection IsProbabilityMeasure\nvariable (μ : Measure Ω) [IsProbabilityMeasure μ] [MeasurableSingletonClass S]\n  [MeasurableSingletonClass T]\n\n/-- $$ H[X] - H[X|Y] = I[X : Y] $$ -/\nlemma entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X ; μ] - H[X | Y ; μ] = I[X : Y ; μ] := by\n  rw [mutualInfo_def, chain_rule _ hX hY, add_comm, add_sub_add_left_eq_sub]\n\n/-- $$ H[X|Y] ≤ H[X] $$ -/\nlemma condEntropy_le_entropy (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] ≤ H[X ; μ] :=\n  sub_nonneg.1 $ by rw [entropy_sub_condEntropy _ hX hY]; exact mutualInfo_nonneg hX hY _\n\n/-- $H[X | Y, Z] \\leq H[X | Z]$ -/\nlemma entropy_submodular (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] ≤ H[X | Z ; μ] := by\n  rw [condEntropy_eq_kernel_entropy hX hZ, condEntropy_two_eq_kernel_entropy hX hY hZ]\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  refine (kernel.entropy_condKernel_le_entropy_snd ?_).trans_eq ?_\n  . apply kernel.aefiniteKernelSupport_condDistrib\n    all_goals measurability\n  exact kernel.entropy_congr (condDistrib_snd_ae_eq hY hX hZ _)\n\n/-- Data-processing inequality for the conditional entropy:\n$$ H[Y|f(X)] \\geq H[Y|X]$$\nTo upgrade this to equality, see `condEntropy_of_injective'` -/\nlemma condEntropy_comp_ge [FiniteRange X] [FiniteRange Y] (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (f : S → U) : H[Y | f ∘ X ; μ] ≥ H[Y | X; μ] := by\n  have h_joint : H[⟨Y, ⟨X, f ∘ X⟩⟩ ; μ] = H[⟨Y, X⟩ ; μ] := by\n    let g : T × S → T × S × U := fun (y, x) ↦ (y, (x, f x))\n    show H[g ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n    refine entropy_comp_of_injective μ (by exact Measurable.prod hY hX) g (fun _ _ h => ?_)\n    repeat rewrite [Prod.mk.injEq] at h\n    exact Prod.ext h.1 h.2.1\n  have hZ : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  rewrite [chain_rule'' μ hY hX, ← entropy_prod_comp hX μ f, ← h_joint,\n    ← chain_rule'' μ hY (Measurable.prod (by exact hX) (by exact hZ))]\n  exact entropy_submodular μ hY hX hZ\n\n","proof":":= by\n  rw [chain_rule _ hX (hY.prod_mk hZ), chain_rule _ hX hZ, chain_rule _ hY hZ]\n  ring_nf\n  exact add_le_add le_rfl (entropy_submodular _ hX hY hZ)","declId":"PFR.ForMathlib.Entropy.Basic.925_0.C2VYVgxGWlr8Qwd","decl":"/-- The submodularity inequality:\n$$ H[X, Y, Z] + H[Z] \\leq H[X, Z] + H[Y, Z].$$ -/\nlemma entropy_triple_add_entropy_le (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] + H[Z ; μ] ≤ H[⟨X, Z⟩ ; μ] + H[⟨Y, Z⟩ ; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 := (indepFun_const c).mutualInfo_eq_zero hX measurable_const\n\nlemma IndepFun.condEntropy_eq_entropy {μ : Measure Ω} (h : IndepFun X Y μ)\n    (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[X ; μ] := by\n  have := h.mutualInfo_eq_zero hX hY\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY] at this\n  linarith\n\n/-- $H[X, Y] = H[X] + H[Y]$ if and only if $X, Y$ are independent. -/\nlemma entropy_pair_eq_add (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y ; μ] ↔ IndepFun X Y μ := by\n  rw [eq_comm, ← sub_eq_zero, ← mutualInfo_eq_zero hX hY]; rfl\n\n/-- If $X, Y$ are independent, then $H[X, Y] = H[X] + H[Y]$. -/\nprotected alias ⟨_, IndepFun.entropy_pair_eq_add⟩ := entropy_pair_eq_add\n\n/-- The conditional mutual information $I[X : Y| Z]$ is the mutual information of $X| Z=z$ and\n$Y| Z=z$, integrated over $z$. -/\nnoncomputable\ndef condMutualInfo (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω := by volume_tac) :\n    ℝ := (μ.map Z)[fun z ↦ H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]]\n\nlemma condMutualInfo_def (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω) :\n    condMutualInfo X Y Z μ = (μ.map Z)[fun z ↦\n      H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]] := rfl\n\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \";\" μ \"]\" => condMutualInfo X Y Z μ\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \"]\" => condMutualInfo X Y Z volume\n\n/-- The conditional mutual information agrees with the information of the conditional kernel.\n-/\nlemma condMutualInfo_eq_kernel_mutualInfo\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = Ik[condDistrib (⟨X, Y⟩) Z μ, μ.map Z] := by\n  rcases finiteSupport_of_finiteRange (μ := μ) (X := Z) with ⟨A, hA⟩\n  simp_rw [condMutualInfo_def, entropy_def, kernel.mutualInfo, kernel.entropy,\n    integral_eq_sum' _ hA, smul_eq_mul, mul_sub, mul_add, Finset.sum_sub_distrib, Finset.sum_add_distrib]\n  congr with x\n  · have h := condDistrib_fst_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hX hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · have h := condDistrib_snd_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hY hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [condDistrib_apply (hX.prod_mk hY) hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n\nlemma condMutualInfo_eq_integral_mutualInfo :\n    I[X : Y | Z ; μ] = (μ.map Z)[fun z ↦ I[X : Y ; μ[| Z ⁻¹' {z}]]] := rfl\n\nlemma condMutualInfo_eq_sum [IsFiniteMeasure μ] (hZ : Measurable Z) [FiniteRange Z] :\n    I[X : Y | Z ; μ] = ∑ z in FiniteRange.toFinset Z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_integral_mutualInfo, integral_eq_sum' _ (FiniteRange.null_of_compl _ Z)]\n  congr 1 with z\n  rw [map_apply hZ (MeasurableSet.singleton z)]\n  rfl\n\n/-- A variant of `condMutualInfo_eq_sum` when `Z` has finite codomain. -/\nlemma condMutualInfo_eq_sum' [IsFiniteMeasure μ] (hZ : Measurable Z) [Fintype U] :\n    I[X : Y | Z ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_sum hZ]\n  apply Finset.sum_subset\n  . simp\n  intro z _ hz\n  have : Z ⁻¹' {z} = ∅ := by\n    ext ω\n    simp at hz\n    simp [hz]\n  simp [this]\n\n/-- $I[X : Y | Z] = I[Y : X | Z]$. -/\nlemma condMutualInfo_comm\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω) :\n    I[X : Y | Z ; μ] = I[Y : X | Z ; μ] := by\n  simp_rw [condMutualInfo_def, add_comm, entropy_comm hX hY]\n\n/-- Conditional information is non-nonegative. -/\nlemma condMutualInfo_nonneg\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y | Z ; μ] := by\n  refine integral_nonneg (fun z ↦ ?_)\n  exact mutualInfo_nonneg hX hY _\n\n/-- $$ I[X : Y| Z] = H[X| Z] + H[Y| Z] - H[X, Y| Z].$$ -/\nlemma condMutualInfo_eq (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = H[X | Z ; μ] + H[Y | Z; μ] - H[⟨X, Y⟩ | Z ; μ] := by\n  rw [condMutualInfo_eq_kernel_mutualInfo hX hY hZ, kernel.mutualInfo,\n    kernel.entropy_congr (condDistrib_fst_ae_eq hX hY hZ _),\n    kernel.entropy_congr (condDistrib_snd_ae_eq hX hY hZ _),\n    condEntropy_eq_kernel_entropy hX hZ, condEntropy_eq_kernel_entropy hY hZ,\n    condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ]\n\n/-- $$ I[X : Y| Z] = H[X| Z] - H[X|Y, Z].$$ -/\nlemma condMutualInfo_eq' (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = H[X | Z ; μ] - H[X | ⟨Y, Z⟩ ; μ] := by\n  rw [condMutualInfo_eq hX hY hZ, cond_chain_rule _ hX hY hZ]\n  ring\n\n/-- If $f(Z, X)$ is injective for each fixed $Z$, then $I[f(Z, X) : Y| Z] = I[X : Y| Z]$.-/\nlemma condMutualInfo_of_inj_map [IsProbabilityMeasure μ]\n  (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n  {V : Type*} [Nonempty V] [MeasurableSpace V] [MeasurableSingletonClass V] [Countable V]\n  (f : U → S → V) (hf : ∀ t, Function.Injective (f t)) [FiniteRange Z] :\n    I[fun ω ↦ f (Z ω) (X ω) : Y | Z ; μ] =\n    I[X : Y | Z ; μ] := by\n  have hM : Measurable (Function.uncurry f ∘ ⟨Z, X⟩) :=\n    (measurable_of_countable _).comp (hZ.prod_mk hX)\n  have hM : Measurable fun ω ↦ f (Z ω) (X ω) := hM\n  rw [condMutualInfo_eq hM hY hZ, condMutualInfo_eq hX hY hZ]\n  let g : U → (S × T) → (V × T) := fun z (x, y) ↦ (f z x, y)\n  have hg : ∀ t, Function.Injective (g t) :=\n    fun _ _ _ h ↦ Prod.ext_iff.2 ⟨hf _ (Prod.ext_iff.1 h).1, (Prod.ext_iff.1 h).2⟩\n  rw [← condEntropy_of_injective μ (hX.prod_mk hY) hZ g hg, ← condEntropy_of_injective μ hX hZ _ hf]\n\nlemma condEntropy_prod_eq_of_indepFun [Fintype T] [Fintype U] [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X]\n    (h : IndepFun (⟨X, Y⟩) Z μ) :\n    H[X | ⟨Y, Z⟩ ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_prod_eq_sum _ hY hZ]\n  have : H[X | Y ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ] := by\n    rw [← Finset.sum_mul, sum_measure_preimage_singleton' μ hZ, one_mul]\n  rw [this]\n  congr with w\n  rcases eq_or_ne (μ (Z ⁻¹' {w})) 0 with hw|hw\n  · simp [hw]\n  congr 1\n  have : IsProbabilityMeasure (μ[|Z ⁻¹' {w}]) := cond_isProbabilityMeasure μ hw\n  apply IdentDistrib.condEntropy_eq hX hY hX hY\n  exact (h.identDistrib_cond (MeasurableSet.singleton w) (hX.prod_mk hY) hZ hw).symm\n\nsection IsProbabilityMeasure\nvariable (μ : Measure Ω) [IsProbabilityMeasure μ] [MeasurableSingletonClass S]\n  [MeasurableSingletonClass T]\n\n/-- $$ H[X] - H[X|Y] = I[X : Y] $$ -/\nlemma entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X ; μ] - H[X | Y ; μ] = I[X : Y ; μ] := by\n  rw [mutualInfo_def, chain_rule _ hX hY, add_comm, add_sub_add_left_eq_sub]\n\n/-- $$ H[X|Y] ≤ H[X] $$ -/\nlemma condEntropy_le_entropy (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] ≤ H[X ; μ] :=\n  sub_nonneg.1 $ by rw [entropy_sub_condEntropy _ hX hY]; exact mutualInfo_nonneg hX hY _\n\n/-- $H[X | Y, Z] \\leq H[X | Z]$ -/\nlemma entropy_submodular (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] ≤ H[X | Z ; μ] := by\n  rw [condEntropy_eq_kernel_entropy hX hZ, condEntropy_two_eq_kernel_entropy hX hY hZ]\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  refine (kernel.entropy_condKernel_le_entropy_snd ?_).trans_eq ?_\n  . apply kernel.aefiniteKernelSupport_condDistrib\n    all_goals measurability\n  exact kernel.entropy_congr (condDistrib_snd_ae_eq hY hX hZ _)\n\n/-- Data-processing inequality for the conditional entropy:\n$$ H[Y|f(X)] \\geq H[Y|X]$$\nTo upgrade this to equality, see `condEntropy_of_injective'` -/\nlemma condEntropy_comp_ge [FiniteRange X] [FiniteRange Y] (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (f : S → U) : H[Y | f ∘ X ; μ] ≥ H[Y | X; μ] := by\n  have h_joint : H[⟨Y, ⟨X, f ∘ X⟩⟩ ; μ] = H[⟨Y, X⟩ ; μ] := by\n    let g : T × S → T × S × U := fun (y, x) ↦ (y, (x, f x))\n    show H[g ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n    refine entropy_comp_of_injective μ (by exact Measurable.prod hY hX) g (fun _ _ h => ?_)\n    repeat rewrite [Prod.mk.injEq] at h\n    exact Prod.ext h.1 h.2.1\n  have hZ : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  rewrite [chain_rule'' μ hY hX, ← entropy_prod_comp hX μ f, ← h_joint,\n    ← chain_rule'' μ hY (Measurable.prod (by exact hX) (by exact hZ))]\n  exact entropy_submodular μ hY hX hZ\n\n/-- The submodularity inequality:\n$$ H[X, Y, Z] + H[Z] \\leq H[X, Z] + H[Y, Z].$$ -/\nlemma entropy_triple_add_entropy_le (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] + H[Z ; μ] ≤ H[⟨X, Z⟩ ; μ] + H[⟨Y, Z⟩ ; μ] := by\n  rw [chain_rule _ hX (hY.prod_mk hZ), chain_rule _ hX hZ, chain_rule _ hY hZ]\n  ring_nf\n  exact add_le_add le_rfl (entropy_submodular _ hX hY hZ)\n\nvariable {μ : Measure Ω}\n\n","proof":":= by\n  rw [condIndepFun_iff, condMutualInfo_eq_integral_mutualInfo, integral_eq_zero_iff_of_nonneg]\n  . dsimp\n    have : (fun x ↦ I[X : Y;μ[| Z ⁻¹' {x}]]) =ᵐ[μ.map Z] 0 ↔ ∀ᵐ z ∂(μ.map Z), I[X : Y ; μ[| Z ⁻¹' {z}]] = 0 := by rfl\n    rw [this]\n    apply Filter.eventually_congr\n    rw [ae_iff_of_countable]\n    intro z hz\n    rw [Measure.map_apply hZ (measurableSet_singleton z)] at hz\n    have : IsProbabilityMeasure (μ[| Z ⁻¹' {z}]) := cond_isProbabilityMeasure μ hz\n    exact mutualInfo_eq_zero hX hY\n  . dsimp\n    rw [Pi.le_def]\n    intro z; simp\n    by_cases hz : μ (Z ⁻¹' {z}) = 0\n    · have : μ[| Z ⁻¹' {z}] = 0 := cond_eq_zero_of_measure_eq_zero hz\n      simp [this]\n      rw [mutualInfo_def]\n      simp\n    exact mutualInfo_nonneg hX hY _\n  simp\n  exact integrable_of_finiteSupport _","declId":"PFR.ForMathlib.Entropy.Basic.935_0.C2VYVgxGWlr8Qwd","decl":"/-- $I[X : Y| Z]=0$ iff $X, Y$ are conditionally independent over $Z$. -/\nlemma condMutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = 0 ↔ CondIndepFun X Y Z μ "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 := (indepFun_const c).mutualInfo_eq_zero hX measurable_const\n\nlemma IndepFun.condEntropy_eq_entropy {μ : Measure Ω} (h : IndepFun X Y μ)\n    (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[X ; μ] := by\n  have := h.mutualInfo_eq_zero hX hY\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY] at this\n  linarith\n\n/-- $H[X, Y] = H[X] + H[Y]$ if and only if $X, Y$ are independent. -/\nlemma entropy_pair_eq_add (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y ; μ] ↔ IndepFun X Y μ := by\n  rw [eq_comm, ← sub_eq_zero, ← mutualInfo_eq_zero hX hY]; rfl\n\n/-- If $X, Y$ are independent, then $H[X, Y] = H[X] + H[Y]$. -/\nprotected alias ⟨_, IndepFun.entropy_pair_eq_add⟩ := entropy_pair_eq_add\n\n/-- The conditional mutual information $I[X : Y| Z]$ is the mutual information of $X| Z=z$ and\n$Y| Z=z$, integrated over $z$. -/\nnoncomputable\ndef condMutualInfo (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω := by volume_tac) :\n    ℝ := (μ.map Z)[fun z ↦ H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]]\n\nlemma condMutualInfo_def (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω) :\n    condMutualInfo X Y Z μ = (μ.map Z)[fun z ↦\n      H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]] := rfl\n\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \";\" μ \"]\" => condMutualInfo X Y Z μ\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \"]\" => condMutualInfo X Y Z volume\n\n/-- The conditional mutual information agrees with the information of the conditional kernel.\n-/\nlemma condMutualInfo_eq_kernel_mutualInfo\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = Ik[condDistrib (⟨X, Y⟩) Z μ, μ.map Z] := by\n  rcases finiteSupport_of_finiteRange (μ := μ) (X := Z) with ⟨A, hA⟩\n  simp_rw [condMutualInfo_def, entropy_def, kernel.mutualInfo, kernel.entropy,\n    integral_eq_sum' _ hA, smul_eq_mul, mul_sub, mul_add, Finset.sum_sub_distrib, Finset.sum_add_distrib]\n  congr with x\n  · have h := condDistrib_fst_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hX hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · have h := condDistrib_snd_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hY hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [condDistrib_apply (hX.prod_mk hY) hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n\nlemma condMutualInfo_eq_integral_mutualInfo :\n    I[X : Y | Z ; μ] = (μ.map Z)[fun z ↦ I[X : Y ; μ[| Z ⁻¹' {z}]]] := rfl\n\nlemma condMutualInfo_eq_sum [IsFiniteMeasure μ] (hZ : Measurable Z) [FiniteRange Z] :\n    I[X : Y | Z ; μ] = ∑ z in FiniteRange.toFinset Z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_integral_mutualInfo, integral_eq_sum' _ (FiniteRange.null_of_compl _ Z)]\n  congr 1 with z\n  rw [map_apply hZ (MeasurableSet.singleton z)]\n  rfl\n\n/-- A variant of `condMutualInfo_eq_sum` when `Z` has finite codomain. -/\nlemma condMutualInfo_eq_sum' [IsFiniteMeasure μ] (hZ : Measurable Z) [Fintype U] :\n    I[X : Y | Z ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_sum hZ]\n  apply Finset.sum_subset\n  . simp\n  intro z _ hz\n  have : Z ⁻¹' {z} = ∅ := by\n    ext ω\n    simp at hz\n    simp [hz]\n  simp [this]\n\n/-- $I[X : Y | Z] = I[Y : X | Z]$. -/\nlemma condMutualInfo_comm\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω) :\n    I[X : Y | Z ; μ] = I[Y : X | Z ; μ] := by\n  simp_rw [condMutualInfo_def, add_comm, entropy_comm hX hY]\n\n/-- Conditional information is non-nonegative. -/\nlemma condMutualInfo_nonneg\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y | Z ; μ] := by\n  refine integral_nonneg (fun z ↦ ?_)\n  exact mutualInfo_nonneg hX hY _\n\n/-- $$ I[X : Y| Z] = H[X| Z] + H[Y| Z] - H[X, Y| Z].$$ -/\nlemma condMutualInfo_eq (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = H[X | Z ; μ] + H[Y | Z; μ] - H[⟨X, Y⟩ | Z ; μ] := by\n  rw [condMutualInfo_eq_kernel_mutualInfo hX hY hZ, kernel.mutualInfo,\n    kernel.entropy_congr (condDistrib_fst_ae_eq hX hY hZ _),\n    kernel.entropy_congr (condDistrib_snd_ae_eq hX hY hZ _),\n    condEntropy_eq_kernel_entropy hX hZ, condEntropy_eq_kernel_entropy hY hZ,\n    condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ]\n\n/-- $$ I[X : Y| Z] = H[X| Z] - H[X|Y, Z].$$ -/\nlemma condMutualInfo_eq' (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = H[X | Z ; μ] - H[X | ⟨Y, Z⟩ ; μ] := by\n  rw [condMutualInfo_eq hX hY hZ, cond_chain_rule _ hX hY hZ]\n  ring\n\n/-- If $f(Z, X)$ is injective for each fixed $Z$, then $I[f(Z, X) : Y| Z] = I[X : Y| Z]$.-/\nlemma condMutualInfo_of_inj_map [IsProbabilityMeasure μ]\n  (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n  {V : Type*} [Nonempty V] [MeasurableSpace V] [MeasurableSingletonClass V] [Countable V]\n  (f : U → S → V) (hf : ∀ t, Function.Injective (f t)) [FiniteRange Z] :\n    I[fun ω ↦ f (Z ω) (X ω) : Y | Z ; μ] =\n    I[X : Y | Z ; μ] := by\n  have hM : Measurable (Function.uncurry f ∘ ⟨Z, X⟩) :=\n    (measurable_of_countable _).comp (hZ.prod_mk hX)\n  have hM : Measurable fun ω ↦ f (Z ω) (X ω) := hM\n  rw [condMutualInfo_eq hM hY hZ, condMutualInfo_eq hX hY hZ]\n  let g : U → (S × T) → (V × T) := fun z (x, y) ↦ (f z x, y)\n  have hg : ∀ t, Function.Injective (g t) :=\n    fun _ _ _ h ↦ Prod.ext_iff.2 ⟨hf _ (Prod.ext_iff.1 h).1, (Prod.ext_iff.1 h).2⟩\n  rw [← condEntropy_of_injective μ (hX.prod_mk hY) hZ g hg, ← condEntropy_of_injective μ hX hZ _ hf]\n\nlemma condEntropy_prod_eq_of_indepFun [Fintype T] [Fintype U] [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X]\n    (h : IndepFun (⟨X, Y⟩) Z μ) :\n    H[X | ⟨Y, Z⟩ ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_prod_eq_sum _ hY hZ]\n  have : H[X | Y ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ] := by\n    rw [← Finset.sum_mul, sum_measure_preimage_singleton' μ hZ, one_mul]\n  rw [this]\n  congr with w\n  rcases eq_or_ne (μ (Z ⁻¹' {w})) 0 with hw|hw\n  · simp [hw]\n  congr 1\n  have : IsProbabilityMeasure (μ[|Z ⁻¹' {w}]) := cond_isProbabilityMeasure μ hw\n  apply IdentDistrib.condEntropy_eq hX hY hX hY\n  exact (h.identDistrib_cond (MeasurableSet.singleton w) (hX.prod_mk hY) hZ hw).symm\n\nsection IsProbabilityMeasure\nvariable (μ : Measure Ω) [IsProbabilityMeasure μ] [MeasurableSingletonClass S]\n  [MeasurableSingletonClass T]\n\n/-- $$ H[X] - H[X|Y] = I[X : Y] $$ -/\nlemma entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X ; μ] - H[X | Y ; μ] = I[X : Y ; μ] := by\n  rw [mutualInfo_def, chain_rule _ hX hY, add_comm, add_sub_add_left_eq_sub]\n\n/-- $$ H[X|Y] ≤ H[X] $$ -/\nlemma condEntropy_le_entropy (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] ≤ H[X ; μ] :=\n  sub_nonneg.1 $ by rw [entropy_sub_condEntropy _ hX hY]; exact mutualInfo_nonneg hX hY _\n\n/-- $H[X | Y, Z] \\leq H[X | Z]$ -/\nlemma entropy_submodular (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] ≤ H[X | Z ; μ] := by\n  rw [condEntropy_eq_kernel_entropy hX hZ, condEntropy_two_eq_kernel_entropy hX hY hZ]\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  refine (kernel.entropy_condKernel_le_entropy_snd ?_).trans_eq ?_\n  . apply kernel.aefiniteKernelSupport_condDistrib\n    all_goals measurability\n  exact kernel.entropy_congr (condDistrib_snd_ae_eq hY hX hZ _)\n\n/-- Data-processing inequality for the conditional entropy:\n$$ H[Y|f(X)] \\geq H[Y|X]$$\nTo upgrade this to equality, see `condEntropy_of_injective'` -/\nlemma condEntropy_comp_ge [FiniteRange X] [FiniteRange Y] (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (f : S → U) : H[Y | f ∘ X ; μ] ≥ H[Y | X; μ] := by\n  have h_joint : H[⟨Y, ⟨X, f ∘ X⟩⟩ ; μ] = H[⟨Y, X⟩ ; μ] := by\n    let g : T × S → T × S × U := fun (y, x) ↦ (y, (x, f x))\n    show H[g ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n    refine entropy_comp_of_injective μ (by exact Measurable.prod hY hX) g (fun _ _ h => ?_)\n    repeat rewrite [Prod.mk.injEq] at h\n    exact Prod.ext h.1 h.2.1\n  have hZ : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  rewrite [chain_rule'' μ hY hX, ← entropy_prod_comp hX μ f, ← h_joint,\n    ← chain_rule'' μ hY (Measurable.prod (by exact hX) (by exact hZ))]\n  exact entropy_submodular μ hY hX hZ\n\n/-- The submodularity inequality:\n$$ H[X, Y, Z] + H[Z] \\leq H[X, Z] + H[Y, Z].$$ -/\nlemma entropy_triple_add_entropy_le (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] + H[Z ; μ] ≤ H[⟨X, Z⟩ ; μ] + H[⟨Y, Z⟩ ; μ] := by\n  rw [chain_rule _ hX (hY.prod_mk hZ), chain_rule _ hX hZ, chain_rule _ hY hZ]\n  ring_nf\n  exact add_le_add le_rfl (entropy_submodular _ hX hY hZ)\n\nvariable {μ : Measure Ω}\n\n/-- $I[X : Y| Z]=0$ iff $X, Y$ are conditionally independent over $Z$. -/\nlemma condMutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = 0 ↔ CondIndepFun X Y Z μ := by\n  rw [condIndepFun_iff, condMutualInfo_eq_integral_mutualInfo, integral_eq_zero_iff_of_nonneg]\n  . dsimp\n    have : (fun x ↦ I[X : Y;μ[| Z ⁻¹' {x}]]) =ᵐ[μ.map Z] 0 ↔ ∀ᵐ z ∂(μ.map Z), I[X : Y ; μ[| Z ⁻¹' {z}]] = 0 := by rfl\n    rw [this]\n    apply Filter.eventually_congr\n    rw [ae_iff_of_countable]\n    intro z hz\n    rw [Measure.map_apply hZ (measurableSet_singleton z)] at hz\n    have : IsProbabilityMeasure (μ[| Z ⁻¹' {z}]) := cond_isProbabilityMeasure μ hz\n    exact mutualInfo_eq_zero hX hY\n  . dsimp\n    rw [Pi.le_def]\n    intro z; simp\n    by_cases hz : μ (Z ⁻¹' {z}) = 0\n    · have : μ[| Z ⁻¹' {z}] = 0 := cond_eq_zero_of_measure_eq_zero hz\n      simp [this]\n      rw [mutualInfo_def]\n      simp\n    exact mutualInfo_nonneg hX hY _\n  simp\n  exact integrable_of_finiteSupport _\n\n","proof":":= by\n  have hI : I[X : Y | Z ; μ] = 0 := (condMutualInfo_eq_zero hX hY hZ).mpr h\n  rw [condMutualInfo_eq hX hY hZ] at hI\n  rw [entropy_assoc hX hY hZ, chain_rule _ (hX.prod_mk hY) hZ, chain_rule _ hX hZ, chain_rule _ hY hZ]\n  linarith [hI]","declId":"PFR.ForMathlib.Entropy.Basic.961_0.C2VYVgxGWlr8Qwd","decl":"/-- If $X, Y$ are conditionally independent over $Z$, then $H[X, Y, Z] = H[X, Z] + H[Y, Z] - H[Z]$. -/\nlemma ent_of_cond_indep (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n     (h : CondIndepFun X Y Z μ) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n     H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨X, Z⟩; μ] + H[⟨Y, Z⟩; μ] - H[Z; μ] "}
{"srcUpToDecl":"import PFR.ForMathlib.Entropy.Kernel.MutualInfo\nimport PFR.ForMathlib.Entropy.Kernel.Basic\nimport PFR.ForMathlib.Uniform\nimport PFR.Mathlib.Probability.Independence.Conditional\n\n/-!\n# Entropy and conditional entropy\n\n## Main definitions\n\n* `entropy`: entropy of a random variable, defined as `measureEntropy (volume.map X)`\n* `condEntropy`: conditional entropy of a random variable `X` w.r.t. another one `Y`\n* `mutualInfo`: mutual information of two random variables\n\n## Main statements\n\n* `chain_rule`: $H[⟨X, Y⟩] = H[Y] + H[X | Y]$\n* `entropy_cond_le_entropy`: $H[X | Y] ≤ H[X]$. (Chain rule another way.)\n* `entropy_triple_add_entropy_le`: $H[X, Y, Z] + H[Z] ≤ H[X, Z] + H[Y, Z]$. (Submodularity of entropy.)\n\n## Notations\n\n* `H[X] = entropy X`\n* `H[X | Y ← y] = Hm[(ℙ[|Y ← y]).map X]`\n* `H[X | Y] = condEntropy X Y`, such that `H[X | Y] = (volume.map Y)[fun y ↦ H[X | Y ← y]]`\n* `I[X : Y] = mutualInfo X Y`\n\nAll notations have variants where we can specify the measure (which is otherwise\nsupposed to be `volume`). For example `H[X ; μ]` and `I[X : Y ; μ]` instead of `H[X]` and\n`I[X : Y]` respectively.\n\n-/\n\nopen Function MeasureTheory Measure Real\nopen scoped ENNReal NNReal Topology ProbabilityTheory BigOperators\n\nnamespace ProbabilityTheory\n\nuniverse uΩ uS uT uU\n\nvariable {Ω : Type uΩ} {S : Type uS} {T : Type uT} {U : Type uU} [mΩ : MeasurableSpace Ω]\n  [Countable S] [Countable T] [Countable U]\n  [Nonempty S] [Nonempty T] [Nonempty U]\n  [MeasurableSpace S] [MeasurableSpace T] [MeasurableSpace U]\n  [MeasurableSingletonClass S] [MeasurableSingletonClass T] [MeasurableSingletonClass U]\n  {X : Ω → S} {Y : Ω → T} {Z : Ω → U}\n  {μ : Measure Ω}\n\nsection entropy\n\n/-- Entropy of a random variable with values in a finite measurable space. -/\nnoncomputable\ndef entropy (X : Ω → S) (μ : Measure Ω := by volume_tac) := Hm[μ.map X]\n\n@[inherit_doc entropy] notation3:max \"H[\" X \" ; \" μ \"]\" => entropy X μ\n@[inherit_doc entropy] notation3:max \"H[\" X \"]\" => entropy X volume\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \" ; \" μ \"]\" => entropy X (μ[|Y ← y])\n@[inherit_doc entropy] notation3:max \"H[\" X \" | \" Y \" ← \" y \"]\" => entropy X (ℙ[|Y ← y])\n\n/-- Entropy of a random variable agrees with entropy of its distribution. -/\nlemma entropy_def (X : Ω → S) (μ : Measure Ω) : entropy X μ = Hm[μ.map X] := rfl\n\n/-- Entropy of a random variable is also the kernel entropy of the distribution over a Dirac mass. -/\nlemma entropy_eq_kernel_entropy (X : Ω → S) (μ : Measure Ω) :\n    H[X ; μ] = Hk[kernel.const Unit (μ.map X), Measure.dirac ()] := by\n  simp only [kernel.entropy, kernel.const_apply, integral_const, MeasurableSpace.measurableSet_top,\n    Measure.dirac_apply', Set.mem_univ, Set.indicator_of_mem, Pi.one_apply, ENNReal.one_toReal,\n    smul_eq_mul, one_mul]\n  rfl\n\n/-- Any variable on a zero measure space has zero entropy. -/\n@[simp]\nlemma entropy_zero_measure (X : Ω → S) : H[X ; (0 : Measure Ω)] = 0 := by simp [entropy]\n\n/-- Two variables that agree almost everywhere, have the same entropy. -/\nlemma entropy_congr {X X' : Ω → S} (h : X =ᵐ[μ] X') : H[X ; μ] = H[X' ; μ] := by\n  rw [entropy_def, Measure.map_congr h, entropy_def]\n\n/-- Entropy is always non-negative. -/\nlemma entropy_nonneg (X : Ω → S) (μ : Measure Ω) : 0 ≤ entropy X μ := measureEntropy_nonneg _\n\n/-- Two variables that have the same distribution, have the same entropy. -/\nlemma IdentDistrib.entropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'} {X' : Ω' → S}\n    (h : IdentDistrib X X' μ μ') : H[X ; μ] = H[X' ; μ'] := by\n  simp [entropy_def, h.map_eq]\n\n/-- Entropy is at most the logarithm of the cardinality of the range. -/\nlemma entropy_le_log_card [Fintype S] (X : Ω → S) (μ : Measure Ω) : H[X ; μ] ≤ log (Fintype.card S) :=\n  measureEntropy_le_log_card _\n\n/-- Entropy is at most the logarithm of the cardinality of a set in which X almost surely takes values in. -/\nlemma entropy_le_log_card_of_mem {A : Finset S} {μ : Measure Ω} {X : Ω → S}\n    (hX : Measurable X) (h : ∀ᵐ ω ∂μ, X ω ∈ A) :\n    H[X ; μ] ≤ log (Nat.card A) := by\n  apply measureEntropy_le_log_card_of_mem\n  rwa [Measure.map_apply hX (measurableSet_discrete _)]\n\n/-- $H[X] = \\sum_s P[X=s] \\log \\frac{1}{P[X=s]}$. -/\nlemma entropy_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog (μ.map X {x}).toReal := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  rw [entropy_def, measureEntropy_of_isProbabilityMeasure]\n\nlemma entropy_eq_sum' (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    entropy X μ = ∑' x, negMulLog ((μ.map X).real {x}) := by\n  have : IsProbabilityMeasure (Measure.map X μ) := isProbabilityMeasure_map hX.aemeasurable\n  simp only [entropy_def, measureEntropy_of_isProbabilityMeasure, Measure.real]\n\nlemma entropy_eq_sum_finset (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog (μ.map X {x}).toReal := by\n  rw [entropy_eq_sum hX]\n  convert tsum_eq_sum ?_\n  intro s hs\n  convert negMulLog_zero\n  convert ENNReal.zero_toReal\n  convert measure_mono_null ?_ hA\n  simp [hs]\n\nlemma entropy_eq_sum_finset' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] {A : Finset S} (hA : (μ.map X) Aᶜ = 0):\n    entropy X μ = ∑ x in A, negMulLog ((μ.map X).real {x}) := entropy_eq_sum_finset hX hA\n\nlemma entropy_eq_sum_finiteRange (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog (μ.map X {x}).toReal :=  entropy_eq_sum_finset hX (A := FiniteRange.toFinset X) (full_measure_of_finiteRange hX)\n\nlemma entropy_eq_sum_finiteRange' (hX : Measurable X) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X]:\n    entropy X μ = ∑ x in FiniteRange.toFinset X, negMulLog ((μ.map X).real {x}) :=  entropy_eq_sum_finiteRange hX\n\n/-- $H[X|Y=y] = \\sum_s P[X=s|Y=y] \\log \\frac{1}{P[X=s|Y=y]}$. -/\nlemma entropy_cond_eq_sum (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) :\n    H[X | Y ← y ; μ] = ∑' x, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum hX]\n\nlemma entropy_cond_eq_sum_finiteRange (hX : Measurable X) (μ : Measure Ω) [IsProbabilityMeasure μ] (y : T) [FiniteRange X]:\n    H[X | Y ← y ; μ] = ∑ x in FiniteRange.toFinset X, negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  by_cases hy : μ (Y ⁻¹' {y}) = 0\n  · rw [entropy_def, cond_eq_zero_of_measure_eq_zero hy]\n    simp\n  · have : IsProbabilityMeasure (μ[|Y ← y]) := cond_isProbabilityMeasure _ hy\n    rw [entropy_eq_sum_finiteRange hX]\n\n/-- If $X$, $Y$ are $S$-valued and $T$-valued random variables, and $Y = f(X)$ for\nsome injection $f : S \\to T$, then $H[Y] = H[X]$. One can also use `entropy_of_comp_eq_of_comp` as an alternative if verifying injectivity is fiddly. For the upper bound only, see `entropy_comp_le`. -/\nlemma entropy_comp_of_injective\n    (μ : Measure Ω) (hX : Measurable X) (f : S → T) (hf : Function.Injective f) :\n    H[f ∘ X ; μ] = H[X ; μ] := by\n  have hf_m : Measurable f := measurable_of_countable f\n  rw [entropy_def, ← Measure.map_map hf_m hX, measureEntropy_map_of_injective _ _ hf_m hf,\n    entropy_def]\n\n\n/-- The entropy of any constant is zero. -/\n@[simp] lemma entropy_const [IsProbabilityMeasure μ] (c : S) : H[fun _ ↦ c ; μ] = 0 := by\n  simp [entropy, Measure.map_const]\n\nopen Set\n\nopen Function\n\n/-- If $X$ is uniformly distributed on $H$, then $H[X] = \\log |H|$.\n-/\nlemma IsUniform.entropy_eq (H : Finset S) (X : Ω → S) {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  haveI : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX'.aemeasurable\n  have : ∀ (t : S), negMulLog ((μ.map X).real {t}) = ((μ.map X).real {t}) * log (Nat.card H)\n  · intro t\n    by_cases ht : t ∈ H\n    · simp only [negMulLog, neg_mul, neg_mul_eq_mul_neg, IsUniform.measureReal_preimage_of_mem'\n        hX hX' ht, one_div, log_inv, neg_neg]\n    · simp only [negMulLog, map_measureReal_apply hX' (MeasurableSet.singleton t),\n      IsUniform.measureReal_preimage_of_nmem hX ht, neg_zero, log_zero, mul_zero, zero_mul]\n  rw [entropy_eq_sum_finset' (A := H) hX', Finset.sum_congr rfl (fun t _ ↦ this t), ← Finset.sum_mul,\n    Finset.sum_realMeasure_singleton]\n  . convert one_mul _\n    have := IsUniform.full_measure hX hX'\n    simp at this\n    rw [Measure.real, this]\n    simp\n  rw [Measure.map_apply hX' (by measurability)]\n  exact hX.measure_preimage_compl\n\n\n/-- Variant of `IsUniform.entropy_eq` where `H` is a finite `Set` rather than `Finset`. -/\nlemma IsUniform.entropy_eq' {H : Set S} [Finite H] {X : Ω → S} {μ : Measure Ω} [IsProbabilityMeasure μ]\n    (hX : IsUniform H X μ) (hX' : Measurable X) : H[X ; μ] = log (Nat.card H) := by\n  convert IsUniform.entropy_eq H.toFinite.toFinset X ?_ hX' with x\n  . simp; exact Iff.rfl\n  . infer_instance\n  convert hX\n  simp\n\n/-- If $X$ is $S$-valued random variable, then $H[X] = \\log |S|$ if and only if $X$ is uniformly\ndistributed. -/\nlemma entropy_eq_log_card {X : Ω → S} [Fintype S] (hX : Measurable X) (μ : Measure Ω) [hμ : NeZero μ]\n    [IsFiniteMeasure μ] :\n    entropy X μ = log (Fintype.card S) ↔ ∀ s, μ.map X {s} = μ Set.univ / Fintype.card S := by\n  rcases eq_zero_or_neZero (μ.map X) with h | h\n  . have := Measure.le_map_apply (@Measurable.aemeasurable Ω S _ _ X μ hX) Set.univ\n    simp [h] at this; simp [this] at hμ\n  have : IsFiniteMeasure (μ.map X) := by\n    apply Measure.isFiniteMeasure_map\n  rw [entropy_def, measureEntropy_eq_card_iff_measure_eq, Measure.map_apply hX MeasurableSet.univ]\n  simp\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$.  TODO: remove the probability measure hypothesis, which is unncessary here. -/\nlemma prob_ge_exp_neg_entropy (X : Ω → S) (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) [hX': FiniteRange X] :\n    ∃ s : S, μ.map X {s} ≥ (μ Set.univ) * (rexp (- H[X ; μ])).toNNReal := by\n  let μS := μ.map X\n  let μs s := μS {s}\n  rcases finiteSupport_of_finiteRange (X := X) with ⟨A, hA⟩\n\n  let S_nonzero := A.filter (fun s ↦ μs s ≠ 0)\n\n  set norm := μS A with rw_norm\n  have h_norm : norm = μ Set.univ := by\n    have := measure_add_measure_compl (μ := μS) (s := A) (Finset.measurableSet _)\n    rw [hA, add_zero] at this\n    simp [this, Measure.map_apply hX MeasurableSet.univ]\n\n  let pdf_nn s := norm⁻¹ * μs s\n  let pdf s := (pdf_nn s).toReal\n  let neg_log_pdf s := -log (pdf s)\n\n  rcases Finset.eq_empty_or_nonempty S_nonzero with h_empty | h_nonempty\n  . have h_norm_zero : μ Set.univ = 0 := by\n      have h : ∀ s ∈ A, μs s ≠ 0 → μs s ≠ 0 := fun _ _ h ↦ h\n      rw [← h_norm, rw_norm, ← Finset.sum_measure_singleton, ← Finset.sum_filter_of_ne h,\n        show Finset.filter _ _ = S_nonzero from rfl, h_empty, show Finset.sum ∅ μs = 0 from rfl]\n    use Classical.arbitrary (α := S)\n    rw [h_norm_zero, zero_mul]\n    exact le_of_not_gt ENNReal.not_lt_zero\n\n  rcases exists_or_forall_not (fun s ↦ μ.map X {s} = ∞) with h_infty | h_finite\n  . obtain ⟨s, h_s⟩ := h_infty\n    use s; rw [h_s] ; exact le_top\n\n  rcases eq_zero_or_neZero μ with h_zero_measure | _\n  . use Classical.arbitrary (α := S)\n    rw [h_zero_measure, show (0 : Measure Ω) _ = 0 from rfl, zero_mul]\n    exact zero_le _\n\n  have h_norm_pos : 0 < norm := by\n    rw [h_norm, Measure.measure_univ_pos]\n    exact NeZero.ne μ\n  have h_norm_finite : norm < ∞ := by\n    rw [rw_norm, ← Finset.sum_measure_singleton]\n    exact ENNReal.sum_lt_top (fun s _ ↦ h_finite s)\n  have h_invinvnorm_finite : norm⁻¹⁻¹ ≠ ∞ := by\n    rw [inv_inv]\n    exact LT.lt.ne_top h_norm_finite\n  have h_invnorm_ne_zero : norm⁻¹ ≠ 0 := ENNReal.inv_ne_top.mp h_invinvnorm_finite\n  have h_invnorm_finite : norm⁻¹ ≠ ∞ := by\n    rw [← ENNReal.inv_ne_zero, inv_inv]\n    exact ne_zero_of_lt h_norm_pos\n  have h_pdf_finite : ∀ s, pdf_nn s ≠ ∞ := fun s ↦ ENNReal.mul_ne_top h_invnorm_finite (h_finite s)\n\n  have h_norm_cancel : norm * norm⁻¹ = 1 :=\n    ENNReal.mul_inv_cancel (ne_zero_of_lt h_norm_pos) (LT.lt.ne_top h_norm_finite)\n  have h_pdf1 : (∑ s in A, pdf s) = 1 := by\n    rw [← ENNReal.toReal_sum (fun s _ ↦ h_pdf_finite s), ← Finset.mul_sum,\n      Finset.sum_measure_singleton, mul_comm, h_norm_cancel, ENNReal.one_toReal]\n\n  let ⟨s_max, hs, h_min⟩ := Finset.exists_min_image S_nonzero neg_log_pdf h_nonempty\n  have h_pdf_s_max_pos : 0 < pdf s_max := by\n    rw [Finset.mem_filter] at hs\n    have h_nonzero : pdf s_max ≠ 0 := ENNReal.toReal_ne_zero.mpr\n      ⟨mul_ne_zero h_invnorm_ne_zero hs.2, ENNReal.mul_ne_top h_invnorm_finite (h_finite s_max)⟩\n    exact LE.le.lt_of_ne ENNReal.toReal_nonneg h_nonzero.symm\n\n  use s_max\n  rw [← h_norm, ← one_mul (μ.map X _), ← h_norm_cancel, mul_assoc]\n  apply mul_le_mul_of_nonneg_left _ (le_of_lt h_norm_pos)\n  show ENNReal.ofReal (rexp (-H[X ; μ])) ≤ pdf_nn s_max\n  rw [ENNReal.ofReal_le_iff_le_toReal (h_pdf_finite _),\n    show (pdf_nn _).toReal = pdf _ from rfl, ← Real.exp_log h_pdf_s_max_pos]\n  apply exp_monotone\n  rw [neg_le, ← one_mul (-log _), ← h_pdf1, Finset.sum_mul]\n  let g_lhs s := pdf s * neg_log_pdf s_max\n  let g_rhs s := -pdf s * log (pdf s)\n  suffices : ∑ s in A, g_lhs s ≤ ∑ s in A, g_rhs s\n  . convert this\n    rw [entropy_eq_sum_finset hX hA]\n    congr with s\n    simp [negMulLog, g_rhs]\n    simp at h_norm\n    rw [h_norm]\n    simp\n  have h_lhs : ∀ s, μs s = 0 → g_lhs s = 0 := by {intros _ h; simp [h]}\n  have h_rhs : ∀ s, μs s = 0 → g_rhs s = 0 := by {intros _ h; simp [h]}\n  rw [← Finset.sum_filter_of_ne (fun s _ ↦ (h_lhs s).mt),\n    ← Finset.sum_filter_of_ne (fun s _ ↦ (h_rhs s).mt)]\n  apply Finset.sum_le_sum\n  intros s h_s\n  rw [show g_lhs s = _ * _ from rfl, show g_rhs s = _ * _ from rfl, neg_mul_comm]\n  exact mul_le_mul_of_nonneg_left (h_min s h_s) ENNReal.toReal_nonneg\n\n/-- If $X$ is an $S$-valued random variable, then there exists $s \\in S$ such that\n$P[X=s] \\geq \\exp(-H[X])$. -/\nlemma prob_ge_exp_neg_entropy' {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] (X : Ω → S) (hX : Measurable X) [FiniteRange X] :\n    ∃ s : S, rexp (- H[X ; μ]) ≤ μ.real (X ⁻¹' {s}) := by\n  obtain ⟨s, hs⟩ := prob_ge_exp_neg_entropy X μ hX\n  use s\n  rwa [IsProbabilityMeasure.measure_univ, one_mul, ge_iff_le,\n    (show ENNReal.ofNNReal _ = ENNReal.ofReal _ from rfl),\n    ENNReal.ofReal_le_iff_le_toReal (measure_ne_top _ _), ← Measure.real,\n    map_measureReal_apply hX (MeasurableSet.singleton s)] at hs\n\n/-- If $X$ is an $S$-valued random variable of non-positive entropy, then $X$ is almost surely constant. -/\nlemma const_of_nonpos_entropy {Ω : Type*} [MeasurableSpace Ω] {μ : Measure Ω}\n    [IsProbabilityMeasure μ] {X : Ω → S} (hX : Measurable X) [FiniteRange X] (hent: H[X; μ] ≤ 0):\n    ∃ s : S, μ.real (X ⁻¹' {s}) = 1 := by\n    rcases prob_ge_exp_neg_entropy' (μ := μ) X hX with ⟨ s, hs ⟩\n    use s\n    apply LE.le.antisymm\n    . rw [← IsProbabilityMeasure.measureReal_univ (μ := μ)]\n      exact measureReal_mono (subset_univ _) (by finiteness)\n    refine le_trans ?_ hs\n    simp [hent]\n\n\n/-- $H[X, Y] = H[Y, X]$. -/\nlemma entropy_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩; μ] = H[⟨Y, X⟩ ; μ] := by\n  change H[Prod.swap ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n  exact entropy_comp_of_injective μ (hY.prod_mk hX) Prod.swap Prod.swap_injective\n\n/-- $H[(X, Y), Z] = H[X, (Y, Z)]$. -/\nlemma entropy_assoc (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) (μ : Measure Ω) :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ] := by\n  change H[Equiv.prodAssoc _ _ _ ∘ ⟨⟨X, Y⟩, Z⟩ ; μ] = H[⟨⟨X, Y⟩, Z⟩ ; μ]\n  exact entropy_comp_of_injective μ ((hX.prod_mk hY).prod_mk hZ) _ $ Equiv.injective _\n\n/-- $H[X, f(X)] = H[X]$.-/\n@[simp] lemma entropy_prod_comp (hX : Measurable X) (μ : Measure Ω) (f : S → T) :\n    H[⟨X, f ∘ X⟩; μ] = H[X ; μ] :=\n  entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) fun _ _ ab ↦ (Prod.ext_iff.1 ab).1\n\nend entropy\n\nsection condEntropy\nvariable {X : Ω → S} {Y : Ω → T}\n\n/-- Conditional entropy of a random variable w.r.t. another.\nThis is the expectation under the law of `Y` of the entropy of the law of `X` conditioned on the\nevent `Y = y`. -/\nnoncomputable\ndef condEntropy (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]]\n\nlemma condEntropy_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    condEntropy X Y μ = (μ.map Y)[fun y ↦ H[X | Y ← y ; μ]] := rfl\n\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \" ; \" μ \"]\" => condEntropy X Y μ\n@[inherit_doc condEntropy] notation3:max \"H[\" X \" | \" Y \"]\" => condEntropy X Y volume\n\nlemma condEntropy_eq_zero\n    (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] (t : T) (ht : (μ.map Y {t}).toReal = 0) : H[X | Y ← t ; μ] = 0 := by\n  convert entropy_zero_measure X\n  apply cond_eq_zero_of_measure_eq_zero\n  rw [Measure.map_apply hY (MeasurableSet.singleton t)] at ht\n  rw [← measureReal_eq_zero_iff]\n  exact ht\n\n/-- Conditional entropy of a random variable is equal to the entropy of its conditional kernel. -/\nlemma condEntropy_eq_kernel_entropy\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [IsFiniteMeasure μ] [FiniteRange Y] :\n    H[X | Y ; μ] = Hk[condDistrib X Y μ, μ.map Y] := by\n  rw [condEntropy_def, kernel.entropy]\n  apply integral_congr_finiteSupport\n  intro t ht\n  simp only [ENNReal.toReal_eq_zero_iff, measure_ne_top (μ.map Y), or_false] at ht\n  rw [Measure.map_apply hY (measurableSet_singleton _)] at ht\n  simp only [entropy_def]\n  congr\n  ext S hS\n  rw [condDistrib_apply' hX hY _ _ ht hS, Measure.map_apply hX hS,\n      cond_apply _ (hY (measurableSet_singleton _))]\n\n/-- The law of $(X, Z)$ is the image of the law of $(Z, X)$.-/\nlemma map_prod_comap_swap (hX : Measurable X) (hZ : Measurable Z) (μ : Measure Ω) :\n    (μ.map (fun ω ↦ (X ω, Z ω))).comap Prod.swap = μ.map (fun ω ↦ (Z ω, X ω)) := by\n  ext s hs\n  rw [Measure.map_apply (hZ.prod_mk hX) hs, Measure.comap_apply _ Prod.swap_injective _ _ hs]\n  · rw [Measure.map_apply (hX.prod_mk hZ)]\n    · congr with ω\n      simp only [Set.image_swap_eq_preimage_swap, Set.mem_preimage, Prod.swap_prod_mk]\n    · exact MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' hs\n  · exact fun t ht ↦ MeasurableEquiv.prodComm.measurableEmbedding.measurableSet_image' ht\n\nlemma condEntropy_two_eq_kernel_entropy (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] =\n      Hk[kernel.condKernel (condDistrib (fun a ↦ (Y a, X a)) Z μ),\n        Measure.map Z μ ⊗ₘ kernel.fst (condDistrib (fun a ↦ (Y a, X a)) Z μ)] := by\n  rw [Measure.compProd_congr (condDistrib_fst_ae_eq hY hX hZ μ),\n      map_compProd_condDistrib hY hZ,\n      kernel.entropy_congr (condKernel_condDistrib_ae_eq hY hX hZ μ),\n      ← kernel.entropy_congr (swap_condDistrib_ae_eq hY hX hZ μ)]\n  have : μ.map (fun ω ↦ (Z ω, Y ω)) = (μ.map (fun ω ↦ (Y ω, Z ω))).comap Prod.swap := by\n    rw [map_prod_comap_swap hY hZ]\n  rw [this, condEntropy_eq_kernel_entropy hX (hY.prod_mk hZ), kernel.entropy_comap_swap]\n\n/-- Any random variable on a zero measure space has zero conditional entropy. -/\n@[simp]\nlemma condEntropy_zero_measure (X : Ω → S) (Y : Ω → T) : H[X | Y ; (0 : Measure Ω)] = 0 :=\n  by simp [condEntropy]\n\n/-- Conditional entropy is non-negative. -/\nlemma condEntropy_nonneg (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) : 0 ≤ H[X | Y ; μ] :=\n  integral_nonneg (fun _ ↦ measureEntropy_nonneg _)\n\n/-- Conditional entropy is at most the logarithm of the cardinality of the range. -/\nlemma condEntropy_le_log_card [MeasurableSingletonClass S] [Fintype S]\n    (X : Ω → S) (Y : Ω → T) (hY : Measurable Y) (μ : Measure Ω) [IsProbabilityMeasure μ] :\n    H[X | Y ; μ] ≤ log (Fintype.card S) := by\n  refine (integral_mono_of_nonneg ?_ (integrable_const (log (Fintype.card S))) ?_).trans ?_\n  · exact ae_of_all _ (fun _ ↦ entropy_nonneg _ _)\n  · exact ae_of_all _ (fun _ ↦ entropy_le_log_card _ _)\n  · have : IsProbabilityMeasure (μ.map Y) := isProbabilityMeasure_map hY.aemeasurable\n    simp\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY: Measurable Y) [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ y in FiniteRange.toFinset Y, (μ.map Y {y}).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum' (s := FiniteRange.toFinset Y) _ _]\n  simp_rw [smul_eq_mul]\n  exact full_measure_of_finiteRange hY\n\n/-- $H[X|Y] = \\sum_y P[Y=y] H[X|Y=y]$.-/\nlemma condEntropy_eq_sum_fintype\n    [MeasurableSingletonClass T] (X : Ω → S) (Y : Ω → T) (μ : Measure Ω)\n    [IsFiniteMeasure μ] (hY : Measurable Y) [Fintype T] :\n    H[X | Y ; μ] = ∑ y, (μ (Y ⁻¹' {y})).toReal * H[X | Y ← y ; μ] := by\n  rw [condEntropy_def, integral_eq_sum]\n  simp_rw [smul_eq_mul, Measure.map_apply hY (measurableSet_singleton _)]\n\nlemma condEntropy_prod_eq_sum {X : Ω → S} {Y : Ω → T} {Z : Ω → T'} [MeasurableSpace T']\n    [MeasurableSingletonClass T']\n    (μ : Measure Ω) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsFiniteMeasure μ] [Fintype T] [Fintype T'] :\n    H[X | ⟨Y, Z⟩ ; μ]\n      = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ[|Z ⁻¹' {z}]] := by\n  simp_rw [condEntropy_eq_sum_fintype _ _ _ (hY.prod_mk hZ), condEntropy_eq_sum_fintype _ _ _ hY,\n    Fintype.sum_prod_type_right, Finset.mul_sum, ← mul_assoc, ← ENNReal.toReal_mul]\n  congr with y\n  congr with x\n  have A : (fun a ↦ (Y a, Z a)) ⁻¹' {(x, y)} = Z ⁻¹' {y} ∩ Y ⁻¹' {x} := by\n    ext p; simp [and_comm]\n  congr 2\n  · rw [cond_apply _ (hZ (measurableSet_singleton y)), ← mul_assoc, A]\n    rcases eq_or_ne (μ (Z ⁻¹' {y})) 0 with hy|hy\n    · have : μ (Z ⁻¹' {y} ∩ Y ⁻¹' {x}) = 0 :=\n        le_antisymm ((measure_mono (Set.inter_subset_left _ _)).trans hy.le) bot_le\n      simp [this, hy]\n    · rw [ENNReal.mul_inv_cancel hy (by finiteness), one_mul]\n  · rw [A, cond_cond_eq_cond_inter'' (hZ (measurableSet_singleton y))\n      (hY (measurableSet_singleton x))]\n\n/-- $H[X|Y] = \\sum_y \\sum_x P[Y=y] P[X=x|Y=y] log \\frac{1}{P[X=x|Y=y]}$.-/\nlemma condEntropy_eq_sum_sum [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T} (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ]\n      = ∑ y in FiniteRange.toFinset Y, ∑ x in FiniteRange.toFinset X, (μ.map Y {y}).toReal * negMulLog ((μ[|Y ← y]).map X {x}).toReal := by\n  rw [condEntropy_eq_sum _ _ _ hY]\n  congr with y\n  rw [entropy_cond_eq_sum_finiteRange hX, Finset.mul_sum]\n\n/-- Same as previous lemma, but with a sum over a product space rather than a double sum. -/\nlemma condEntropy_eq_sum_prod [MeasurableSingletonClass T] (hX : Measurable X) {Y : Ω → T}\n    (hY : Measurable Y)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X | Y ; μ] = ∑ p in (FiniteRange.toFinset X) ×ˢ (FiniteRange.toFinset Y),\n      (μ.map Y {p.2}).toReal * negMulLog ((μ[|Y ⁻¹' {p.2}]).map X {p.1}).toReal := by\n  rw [condEntropy_eq_sum_sum hX hY, Finset.sum_product_right]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$ are random variables, and $f : T \\times S → U$ is\n  injective for each fixed $t \\in T$, then $H[f(Y, X)|Y] = H[X|Y]$.\n  Thus for instance $H[X-Y|Y] = H[X|Y]$. -/\nlemma condEntropy_of_injective\n    [MeasurableSingletonClass U] (μ : Measure Ω) [IsFiniteMeasure μ] (hX : Measurable X)\n    (hY : Measurable Y) (f : T → S → U) (hf : ∀ t, Injective (f t)) [FiniteRange Y] :\n    H[(fun ω ↦ f (Y ω) (X ω)) | Y ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_eq_sum _ _ _ hY, condEntropy_eq_sum _ _ _ hY]\n  have : ∀ y, H[fun ω ↦ f (Y ω) (X ω)|Y← y; μ] = H[(f y ∘ X) | Y ← y ; μ] := by\n    intro y\n    refine entropy_congr ?_\n    have : ∀ᵐ ω ∂μ[|Y ← y], Y ω = y := by\n      rw [ae_iff, cond_apply _ (hY (measurableSet_singleton _))]\n      have : {a | ¬Y a = y} = (Y ⁻¹' {y})ᶜ := by ext; simp\n      rw [this, Set.inter_compl_self, measure_empty, mul_zero]\n    filter_upwards [this] with ω hω\n    rw [hω]\n    simp\n  simp_rw [this]\n  congr with y\n  rw [entropy_comp_of_injective _ hX (f y) (hf y)]\n\n/-- A weaker version of the above lemma in which f is independent of Y. -/\nlemma condEntropy_comp_of_injective [MeasurableSingletonClass S] [MeasurableSingletonClass U]\n    (μ : Measure Ω) (hX : Measurable X) (f : S → U) (hf : Injective f) :\n    H[f ∘ X | Y ; μ] = H[X | Y ; μ] :=\n  integral_congr_ae (ae_of_all _ (fun _ ↦ entropy_comp_of_injective _ hX f hf))\n\n/-- $H[X, Y| Z] = H[Y, X| Z]$. -/\nlemma condEntropy_comm {Z : Ω → U}\n    (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    H[⟨X, Y⟩ | Z ; μ] = H[⟨Y, X⟩ | Z; μ] := by\n  change H[⟨X, Y⟩ | Z ; μ] = H[Prod.swap ∘ ⟨X, Y⟩ | Z; μ]\n  exact (condEntropy_comp_of_injective μ (hX.prod_mk hY) Prod.swap Prod.swap_injective).symm\n\nend condEntropy\n\nsection pair\n\n/-- One form of the chain rule : $H[X, Y] = H[X] + H[Y|X]. -/\nlemma chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y]:\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y | X ; μ] := by\n  have : IsProbabilityMeasure (μ.map X) := isProbabilityMeasure_map hX.aemeasurable\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  rw [entropy_eq_kernel_entropy, kernel.chain_rule]\n  simp_rw [← kernel.map_const _ (hX.prod_mk hY), kernel.fst_map_prod _ hX hY, kernel.map_const _ hX,\n    kernel.map_const _ (hX.prod_mk hY)]\n  . congr 1\n    · rw [kernel.entropy, integral_dirac]\n      rfl\n    · simp_rw [condEntropy_eq_kernel_entropy hY hX]\n      have : Measure.dirac () ⊗ₘ kernel.const Unit (μ.map X) = μ.map (fun ω ↦ ((), X ω)) := by\n        ext s _\n        rw [Measure.dirac_unit_compProd_const, Measure.map_map measurable_prod_mk_left hX]\n        congr\n      rw [this, kernel.entropy_congr (condDistrib_const_unit hX hY μ)]\n      have : μ.map (fun ω ↦ ((), X ω)) = (μ.map X).map (Prod.mk ()) := by\n        ext s _\n        rw [Measure.map_map measurable_prod_mk_left hX]\n        rfl\n      rw [this, kernel.entropy_prodMkLeft_unit]\n  · apply kernel.FiniteKernelSupport.aefiniteKernelSupport\n    exact kernel.finiteKernelSupport_of_const _\n\n/-- Another form of the chain rule : $H[X, Y] = H[Y] + H[X|Y]$. -/\nlemma chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n   [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[Y ; μ] + H[X | Y ; μ] := by\n  rw [entropy_comm hX hY, chain_rule' μ hY hX]\n\n/-- Another form of the chain rule : $H[X|Y] = H[X, Y] - H[Y]$. -/\nlemma chain_rule'' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[⟨X, Y⟩ ; μ] - H[Y ; μ] := by\n  rw [chain_rule μ hX hY, add_sub_cancel']\n\n/-- Two pairs of variables that have the same joint distribution, have the same\nconditional entropy. -/\nlemma IdentDistrib.condEntropy_eq {Ω' : Type*} [MeasurableSpace Ω'] {X : Ω → S} {Y : Ω → T}\n    {μ' : Measure Ω'} {X' : Ω' → S} {Y' : Ω' → T} [IsProbabilityMeasure μ] [IsProbabilityMeasure μ']\n    (hX : Measurable X) (hY : Measurable Y) (hX' : Measurable X') (hY' : Measurable Y')\n    (h : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') [FiniteRange X] [FiniteRange Y] [FiniteRange X']\n    [FiniteRange Y'] :\n    H[X | Y ; μ] = H[X' | Y' ; μ'] := by\n  have : IdentDistrib Y Y' μ μ' := h.comp measurable_snd\n  rw [chain_rule'' _ hX hY, chain_rule'' _ hX' hY', h.entropy_eq, this.entropy_eq]\n\n/-- If $X : \\Omega \\to S$ and $Y : \\Omega \\to T$ are random variables, and $f : T \\to U$ is an\ninjection then $H[X|f(Y)] = H[X|Y]$.\n -/\nlemma condEntropy_of_injective' [MeasurableSingletonClass S] (μ : Measure Ω)\n    [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y) (f : T → U) (hf : Injective f)\n    (hfY : Measurable (f ∘ Y)) [FiniteRange X] [FiniteRange Y] :\n    H[X | f ∘ Y ; μ] = H[X | Y ; μ] := by\n  rw [chain_rule'' μ hX hY, chain_rule'' μ hX hfY, chain_rule' μ hX hY, chain_rule' μ hX hfY]\n  congr 1\n  . congr 1\n    exact condEntropy_comp_of_injective μ hY f hf\n  exact entropy_comp_of_injective μ hY f hf\n\n/-- $H[X|f(X)] = H[X] - H[f(X)]$. -/\nlemma condEntropy_comp_self [IsProbabilityMeasure μ] (hX : Measurable X) {f : S → U}\n    (hf : Measurable f) [FiniteRange X] : H[X | f ∘ X ; μ] = H[X ; μ] - H[f ∘ X ; μ] := by\n  rw [chain_rule'' μ hX (hf.comp hX), entropy_prod_comp hX _ f]\n\n/-- If $X : \\Omega \\to S$, $Y : \\Omega \\to T$,$Z : \\Omega \\to U$ are random variables, then\n$$H[X, Y | Z] = H[X | Z] + H[Y|X, Z]$$. -/\nlemma cond_chain_rule' (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[X | Z ; μ] + H[Y | ⟨X, Z⟩ ; μ] := by\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  rw [condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ, kernel.chain_rule]\n  . congr 1\n    . rw [condEntropy_eq_kernel_entropy hX hZ]\n      refine kernel.entropy_congr ?_\n      exact condDistrib_fst_ae_eq hX hY hZ μ\n    · rw [condEntropy_two_eq_kernel_entropy hY hX hZ]\n  exact kernel.aefiniteKernelSupport_condDistrib _ _ μ (by measurability) (by measurability)\n\n/-- $$ H[X, Y | Z] = H[Y | Z] + H[X|Y, Z].$$ -/\nlemma cond_chain_rule (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z]:\n    H[⟨X, Y⟩ | Z ; μ] = H[Y | Z ; μ] + H[X | ⟨Y, Z⟩ ; μ] := by\n    rw [condEntropy_comm hX hY, cond_chain_rule' _ hY hX hZ]\n\n/-- Data-processing inequality for the entropy :\n$$ H[f(X)] \\leq H[X].$$\nTo upgrade this to equality, see `entropy_of_comp_eq_of_comp` or `entropy_comp_of_injective`. -/\nlemma entropy_comp_le (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (f : S → U) [FiniteRange X]:\n    H[f ∘ X ; μ] ≤ H[X ; μ] := by\n  have hfX : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  have : H[X ; μ] = H[⟨X, f ∘ X⟩ ; μ] := by\n    refine (entropy_comp_of_injective μ hX (fun x ↦ (x, f x)) ?_).symm\n    intro x y hxy\n    simp only [Prod.mk.injEq] at hxy\n    exact hxy.1\n  rw [this, chain_rule _ hX hfX]\n  simp only [le_add_iff_nonneg_right]\n  exact condEntropy_nonneg X (f ∘ X) μ\n\n/-- A Schroder-Bernstein type theorem for entropy : if two random variables are functions of each\n  other, then they have the same entropy. Can be used as a substitute for\n  `entropy_comp_of_injective` if one doesn't want to establish the injectivity. -/\nlemma entropy_of_comp_eq_of_comp\n    (μ : Measure Ω) [IsProbabilityMeasure μ] (hX : Measurable X) (hY : Measurable Y)\n    (f : S → T) (g : T → S) (h1 : Y = f ∘ X) (h2 : X = g ∘ Y) [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] = H[Y ; μ] := by\n  have h3 : H[X ; μ] ≤ H[Y ; μ] := by\n    rw [h2]; exact entropy_comp_le μ hY _\n  have h4 : H[Y ; μ] ≤ H[X ; μ] := by\n    rw [h1]; exact entropy_comp_le μ hX _\n  linarith\n\nend pair\n\nsection mutualInfo\n\n/-- The mutual information $I[X : Y]$ of two random variables is defined to be $H[X] + H[Y] - H[X ; Y]$. -/\nnoncomputable\ndef mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω := by volume_tac) : ℝ :=\n  H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ]\n\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \" ; \" μ \"]\" => mutualInfo X Y μ\n@[inherit_doc mutualInfo] notation3:max \"I[\" X \" : \" Y \"]\" => mutualInfo X Y volume\n\nlemma mutualInfo_def (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n  I[X : Y ; μ] = H[X ; μ] + H[Y ; μ] - H[⟨X, Y⟩ ; μ] := rfl\n\n/-- $I[X : Y] = I[Y : X]$. -/\nlemma mutualInfo_comm (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) :\n    I[X : Y ; μ] = I[Y : X ; μ] := by simp_rw [mutualInfo, add_comm, entropy_comm hX hY]\n\nlemma entropy_add_entropy_sub_mutualInfo (X : Ω → S) (Y : Ω → T) (μ : Measure Ω) :\n    H[X ; μ] + H[Y ; μ] - I[X : Y ; μ] = H[⟨X, Y⟩ ; μ] := sub_sub_self _ _\n\n/-- $I[X : Y] = H[X] - H[X|Y]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[X ; μ] - H[X | Y ; μ] := by\n  rw [mutualInfo_def, chain_rule μ hX hY]\n  abel\n\n/-- $I[X : Y] = H[Y] - H[Y | X]$. -/\nlemma mutualInfo_eq_entropy_sub_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    I[X : Y ; μ] = H[Y ; μ] - H[Y | X ; μ] := by\n  rw [mutualInfo_comm hX hY, mutualInfo_eq_entropy_sub_condEntropy hY hX]\n\n/-- $H[X] - I[X : Y] = H[X | Y]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[X ; μ] - I[X : Y ; μ] = H[X | Y ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY, sub_sub_self]\n\n/-- $H[Y] - I[X : Y] = H[Y | X]$. -/\nlemma entropy_sub_mutualInfo_eq_condEntropy' (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y]:\n    H[Y ; μ] - I[X : Y ; μ] = H[Y | X ; μ] := by\n  rw [mutualInfo_eq_entropy_sub_condEntropy' hX hY, sub_sub_self]\n\n/-- Mutual information is non-negative. -/\nlemma mutualInfo_nonneg (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y ; μ] := by\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  exact measureMutualInfo_nonneg\n\n/-- Substituting variables for ones with the same distributions doesn't change the mutual information. -/\nlemma IdentDistrib.mutualInfo_eq {Ω' : Type*} [MeasurableSpace Ω'] {μ' : Measure Ω'}\n    {X' : Ω' → S} {Y' : Ω' → T} (hXY : IdentDistrib (⟨X, Y⟩) (⟨X', Y'⟩) μ μ') :\n      I[X : Y ; μ] = I[X' : Y' ; μ'] := by\n  have hX : IdentDistrib X X' μ μ' := hXY.comp measurable_fst\n  have hY : IdentDistrib Y Y' μ μ' := hXY.comp measurable_snd\n  simp_rw [mutualInfo_def,hX.entropy_eq,hY.entropy_eq,hXY.entropy_eq]\n\n/-- Subadditivity of entropy. -/\nlemma entropy_pair_le_add (hX : Measurable X) (hY : Measurable Y) (μ : Measure Ω) [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] ≤ H[X ; μ] + H[Y ; μ] :=\n  sub_nonneg.1 $ mutualInfo_nonneg hX hY _\n\n/-- $I[X : Y] = 0$ iff $X, Y$ are independent. -/\nlemma mutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    I[X : Y ; μ] = 0 ↔ IndepFun X Y μ := by\n  have : IsProbabilityMeasure (μ.map (⟨X, Y⟩)) :=\n    isProbabilityMeasure_map (hX.prod_mk hY).aemeasurable\n  simp_rw [mutualInfo_def, entropy_def]\n  have h_fst : μ.map X = (μ.map (⟨X, Y⟩)).map Prod.fst := by\n    rw [Measure.map_map measurable_fst (hX.prod_mk hY)]\n    congr\n  have h_snd : μ.map Y = (μ.map (⟨X, Y⟩)).map Prod.snd := by\n    rw [Measure.map_map measurable_snd (hX.prod_mk hY)]\n    congr\n  rw [h_fst, h_snd]\n  convert measureMutualInfo_eq_zero_iff (μ := μ.map (⟨X, Y⟩))\n  rw [indepFun_iff_map_prod_eq_prod_map_map hX.aemeasurable hY.aemeasurable,\n    Measure.ext_iff_measureReal_singleton_finiteSupport]\n  congr! with p\n  convert measureReal_prod_prod (μ := μ.map X) (ν := μ.map Y) {p.1} {p.2}\n  · simp\n  · exact Measure.map_map measurable_fst (hX.prod_mk hY)\n  · exact Measure.map_map measurable_snd (hX.prod_mk hY)\n\nprotected alias ⟨_, IndepFun.mutualInfo_eq_zero⟩ := mutualInfo_eq_zero\n\n/-- The mutual information with a constant is always zero. -/\nlemma mutualInfo_const (hX : Measurable X) (c : T) {μ : Measure Ω} [IsProbabilityMeasure μ] [FiniteRange X] :\n    I[X : fun _ ↦ c ; μ] = 0 := (indepFun_const c).mutualInfo_eq_zero hX measurable_const\n\nlemma IndepFun.condEntropy_eq_entropy {μ : Measure Ω} (h : IndepFun X Y μ)\n    (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] = H[X ; μ] := by\n  have := h.mutualInfo_eq_zero hX hY\n  rw [mutualInfo_eq_entropy_sub_condEntropy hX hY] at this\n  linarith\n\n/-- $H[X, Y] = H[X] + H[Y]$ if and only if $X, Y$ are independent. -/\nlemma entropy_pair_eq_add (hX : Measurable X) (hY : Measurable Y) {μ : Measure Ω}\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[⟨X, Y⟩ ; μ] = H[X ; μ] + H[Y ; μ] ↔ IndepFun X Y μ := by\n  rw [eq_comm, ← sub_eq_zero, ← mutualInfo_eq_zero hX hY]; rfl\n\n/-- If $X, Y$ are independent, then $H[X, Y] = H[X] + H[Y]$. -/\nprotected alias ⟨_, IndepFun.entropy_pair_eq_add⟩ := entropy_pair_eq_add\n\n/-- The conditional mutual information $I[X : Y| Z]$ is the mutual information of $X| Z=z$ and\n$Y| Z=z$, integrated over $z$. -/\nnoncomputable\ndef condMutualInfo (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω := by volume_tac) :\n    ℝ := (μ.map Z)[fun z ↦ H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]]\n\nlemma condMutualInfo_def (X : Ω → S) (Y : Ω → T) (Z : Ω → U) (μ : Measure Ω) :\n    condMutualInfo X Y Z μ = (μ.map Z)[fun z ↦\n      H[X | Z ← z ; μ] + H[Y | Z ← z ; μ] - H[⟨X, Y⟩ | Z ← z ; μ]] := rfl\n\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \";\" μ \"]\" => condMutualInfo X Y Z μ\n@[inherit_doc condMutualInfo]\nnotation3:max \"I[\" X \" : \" Y \"|\" Z \"]\" => condMutualInfo X Y Z volume\n\n/-- The conditional mutual information agrees with the information of the conditional kernel.\n-/\nlemma condMutualInfo_eq_kernel_mutualInfo\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = Ik[condDistrib (⟨X, Y⟩) Z μ, μ.map Z] := by\n  rcases finiteSupport_of_finiteRange (μ := μ) (X := Z) with ⟨A, hA⟩\n  simp_rw [condMutualInfo_def, entropy_def, kernel.mutualInfo, kernel.entropy,\n    integral_eq_sum' _ hA, smul_eq_mul, mul_sub, mul_add, Finset.sum_sub_distrib, Finset.sum_add_distrib]\n  congr with x\n  · have h := condDistrib_fst_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hX hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · have h := condDistrib_snd_ae_eq hX hY hZ μ\n    rw [Filter.EventuallyEq, ae_iff_of_countable] at h\n    specialize h x\n    by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [h hx, condDistrib_apply hY hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n  · by_cases hx : (μ.map Z) {x} = 0\n    · simp [hx]\n    rw [condDistrib_apply (hX.prod_mk hY) hZ]\n    rwa [Measure.map_apply hZ (measurableSet_singleton _)] at hx\n\nlemma condMutualInfo_eq_integral_mutualInfo :\n    I[X : Y | Z ; μ] = (μ.map Z)[fun z ↦ I[X : Y ; μ[| Z ⁻¹' {z}]]] := rfl\n\nlemma condMutualInfo_eq_sum [IsFiniteMeasure μ] (hZ : Measurable Z) [FiniteRange Z] :\n    I[X : Y | Z ; μ] = ∑ z in FiniteRange.toFinset Z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_integral_mutualInfo, integral_eq_sum' _ (FiniteRange.null_of_compl _ Z)]\n  congr 1 with z\n  rw [map_apply hZ (MeasurableSet.singleton z)]\n  rfl\n\n/-- A variant of `condMutualInfo_eq_sum` when `Z` has finite codomain. -/\nlemma condMutualInfo_eq_sum' [IsFiniteMeasure μ] (hZ : Measurable Z) [Fintype U] :\n    I[X : Y | Z ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * I[X : Y ; (μ[|Z ← z])] := by\n  rw [condMutualInfo_eq_sum hZ]\n  apply Finset.sum_subset\n  . simp\n  intro z _ hz\n  have : Z ⁻¹' {z} = ∅ := by\n    ext ω\n    simp at hz\n    simp [hz]\n  simp [this]\n\n/-- $I[X : Y | Z] = I[Y : X | Z]$. -/\nlemma condMutualInfo_comm\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω) :\n    I[X : Y | Z ; μ] = I[Y : X | Z ; μ] := by\n  simp_rw [condMutualInfo_def, add_comm, entropy_comm hX hY]\n\n/-- Conditional information is non-nonegative. -/\nlemma condMutualInfo_nonneg\n    (hX : Measurable X) (hY : Measurable Y) (Z : Ω → U) (μ : Measure Ω)\n    [FiniteRange X] [FiniteRange Y] :\n    0 ≤ I[X : Y | Z ; μ] := by\n  refine integral_nonneg (fun z ↦ ?_)\n  exact mutualInfo_nonneg hX hY _\n\n/-- $$ I[X : Y| Z] = H[X| Z] + H[Y| Z] - H[X, Y| Z].$$ -/\nlemma condMutualInfo_eq (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = H[X | Z ; μ] + H[Y | Z; μ] - H[⟨X, Y⟩ | Z ; μ] := by\n  rw [condMutualInfo_eq_kernel_mutualInfo hX hY hZ, kernel.mutualInfo,\n    kernel.entropy_congr (condDistrib_fst_ae_eq hX hY hZ _),\n    kernel.entropy_congr (condDistrib_snd_ae_eq hX hY hZ _),\n    condEntropy_eq_kernel_entropy hX hZ, condEntropy_eq_kernel_entropy hY hZ,\n    condEntropy_eq_kernel_entropy (hX.prod_mk hY) hZ]\n\n/-- $$ I[X : Y| Z] = H[X| Z] - H[X|Y, Z].$$ -/\nlemma condMutualInfo_eq' (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    (μ : Measure Ω) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = H[X | Z ; μ] - H[X | ⟨Y, Z⟩ ; μ] := by\n  rw [condMutualInfo_eq hX hY hZ, cond_chain_rule _ hX hY hZ]\n  ring\n\n/-- If $f(Z, X)$ is injective for each fixed $Z$, then $I[f(Z, X) : Y| Z] = I[X : Y| Z]$.-/\nlemma condMutualInfo_of_inj_map [IsProbabilityMeasure μ]\n  (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n  {V : Type*} [Nonempty V] [MeasurableSpace V] [MeasurableSingletonClass V] [Countable V]\n  (f : U → S → V) (hf : ∀ t, Function.Injective (f t)) [FiniteRange Z] :\n    I[fun ω ↦ f (Z ω) (X ω) : Y | Z ; μ] =\n    I[X : Y | Z ; μ] := by\n  have hM : Measurable (Function.uncurry f ∘ ⟨Z, X⟩) :=\n    (measurable_of_countable _).comp (hZ.prod_mk hX)\n  have hM : Measurable fun ω ↦ f (Z ω) (X ω) := hM\n  rw [condMutualInfo_eq hM hY hZ, condMutualInfo_eq hX hY hZ]\n  let g : U → (S × T) → (V × T) := fun z (x, y) ↦ (f z x, y)\n  have hg : ∀ t, Function.Injective (g t) :=\n    fun _ _ _ h ↦ Prod.ext_iff.2 ⟨hf _ (Prod.ext_iff.1 h).1, (Prod.ext_iff.1 h).2⟩\n  rw [← condEntropy_of_injective μ (hX.prod_mk hY) hZ g hg, ← condEntropy_of_injective μ hX hZ _ hf]\n\nlemma condEntropy_prod_eq_of_indepFun [Fintype T] [Fintype U] [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X]\n    (h : IndepFun (⟨X, Y⟩) Z μ) :\n    H[X | ⟨Y, Z⟩ ; μ] = H[X | Y ; μ] := by\n  rw [condEntropy_prod_eq_sum _ hY hZ]\n  have : H[X | Y ; μ] = ∑ z, (μ (Z ⁻¹' {z})).toReal * H[X | Y ; μ] := by\n    rw [← Finset.sum_mul, sum_measure_preimage_singleton' μ hZ, one_mul]\n  rw [this]\n  congr with w\n  rcases eq_or_ne (μ (Z ⁻¹' {w})) 0 with hw|hw\n  · simp [hw]\n  congr 1\n  have : IsProbabilityMeasure (μ[|Z ⁻¹' {w}]) := cond_isProbabilityMeasure μ hw\n  apply IdentDistrib.condEntropy_eq hX hY hX hY\n  exact (h.identDistrib_cond (MeasurableSet.singleton w) (hX.prod_mk hY) hZ hw).symm\n\nsection IsProbabilityMeasure\nvariable (μ : Measure Ω) [IsProbabilityMeasure μ] [MeasurableSingletonClass S]\n  [MeasurableSingletonClass T]\n\n/-- $$ H[X] - H[X|Y] = I[X : Y] $$ -/\nlemma entropy_sub_condEntropy (hX : Measurable X) (hY : Measurable Y) [FiniteRange X] [FiniteRange Y] :\n    H[X ; μ] - H[X | Y ; μ] = I[X : Y ; μ] := by\n  rw [mutualInfo_def, chain_rule _ hX hY, add_comm, add_sub_add_left_eq_sub]\n\n/-- $$ H[X|Y] ≤ H[X] $$ -/\nlemma condEntropy_le_entropy (hX : Measurable X) (hY : Measurable Y) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] :\n    H[X | Y ; μ] ≤ H[X ; μ] :=\n  sub_nonneg.1 $ by rw [entropy_sub_condEntropy _ hX hY]; exact mutualInfo_nonneg hX hY _\n\n/-- $H[X | Y, Z] \\leq H[X | Z]$ -/\nlemma entropy_submodular (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    H[X | ⟨Y, Z⟩ ; μ] ≤ H[X | Z ; μ] := by\n  rw [condEntropy_eq_kernel_entropy hX hZ, condEntropy_two_eq_kernel_entropy hX hY hZ]\n  have : IsProbabilityMeasure (μ.map Z) := isProbabilityMeasure_map hZ.aemeasurable\n  refine (kernel.entropy_condKernel_le_entropy_snd ?_).trans_eq ?_\n  . apply kernel.aefiniteKernelSupport_condDistrib\n    all_goals measurability\n  exact kernel.entropy_congr (condDistrib_snd_ae_eq hY hX hZ _)\n\n/-- Data-processing inequality for the conditional entropy:\n$$ H[Y|f(X)] \\geq H[Y|X]$$\nTo upgrade this to equality, see `condEntropy_of_injective'` -/\nlemma condEntropy_comp_ge [FiniteRange X] [FiniteRange Y] (μ : Measure Ω) [IsProbabilityMeasure μ]\n    (hX : Measurable X) (hY : Measurable Y) (f : S → U) : H[Y | f ∘ X ; μ] ≥ H[Y | X; μ] := by\n  have h_joint : H[⟨Y, ⟨X, f ∘ X⟩⟩ ; μ] = H[⟨Y, X⟩ ; μ] := by\n    let g : T × S → T × S × U := fun (y, x) ↦ (y, (x, f x))\n    show H[g ∘ ⟨Y, X⟩ ; μ] = H[⟨Y, X⟩ ; μ]\n    refine entropy_comp_of_injective μ (by exact Measurable.prod hY hX) g (fun _ _ h => ?_)\n    repeat rewrite [Prod.mk.injEq] at h\n    exact Prod.ext h.1 h.2.1\n  have hZ : Measurable (f ∘ X) := (measurable_of_countable _).comp hX\n  rewrite [chain_rule'' μ hY hX, ← entropy_prod_comp hX μ f, ← h_joint,\n    ← chain_rule'' μ hY (Measurable.prod (by exact hX) (by exact hZ))]\n  exact entropy_submodular μ hY hX hZ\n\n/-- The submodularity inequality:\n$$ H[X, Y, Z] + H[Z] \\leq H[X, Z] + H[Y, Z].$$ -/\nlemma entropy_triple_add_entropy_le (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z) [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    H[⟨X, ⟨Y, Z⟩⟩ ; μ] + H[Z ; μ] ≤ H[⟨X, Z⟩ ; μ] + H[⟨Y, Z⟩ ; μ] := by\n  rw [chain_rule _ hX (hY.prod_mk hZ), chain_rule _ hX hZ, chain_rule _ hY hZ]\n  ring_nf\n  exact add_le_add le_rfl (entropy_submodular _ hX hY hZ)\n\nvariable {μ : Measure Ω}\n\n/-- $I[X : Y| Z]=0$ iff $X, Y$ are conditionally independent over $Z$. -/\nlemma condMutualInfo_eq_zero (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n    [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n    I[X : Y | Z ; μ] = 0 ↔ CondIndepFun X Y Z μ := by\n  rw [condIndepFun_iff, condMutualInfo_eq_integral_mutualInfo, integral_eq_zero_iff_of_nonneg]\n  . dsimp\n    have : (fun x ↦ I[X : Y;μ[| Z ⁻¹' {x}]]) =ᵐ[μ.map Z] 0 ↔ ∀ᵐ z ∂(μ.map Z), I[X : Y ; μ[| Z ⁻¹' {z}]] = 0 := by rfl\n    rw [this]\n    apply Filter.eventually_congr\n    rw [ae_iff_of_countable]\n    intro z hz\n    rw [Measure.map_apply hZ (measurableSet_singleton z)] at hz\n    have : IsProbabilityMeasure (μ[| Z ⁻¹' {z}]) := cond_isProbabilityMeasure μ hz\n    exact mutualInfo_eq_zero hX hY\n  . dsimp\n    rw [Pi.le_def]\n    intro z; simp\n    by_cases hz : μ (Z ⁻¹' {z}) = 0\n    · have : μ[| Z ⁻¹' {z}] = 0 := cond_eq_zero_of_measure_eq_zero hz\n      simp [this]\n      rw [mutualInfo_def]\n      simp\n    exact mutualInfo_nonneg hX hY _\n  simp\n  exact integrable_of_finiteSupport _\n\n/-- If $X, Y$ are conditionally independent over $Z$, then $H[X, Y, Z] = H[X, Z] + H[Y, Z] - H[Z]$. -/\nlemma ent_of_cond_indep (hX : Measurable X) (hY : Measurable Y) (hZ : Measurable Z)\n     (h : CondIndepFun X Y Z μ) [IsProbabilityMeasure μ] [FiniteRange X] [FiniteRange Y] [FiniteRange Z] :\n     H[⟨X, ⟨Y, Z⟩⟩ ; μ] = H[⟨X, Z⟩; μ] + H[⟨Y, Z⟩; μ] - H[Z; μ] := by\n  have hI : I[X : Y | Z ; μ] = 0 := (condMutualInfo_eq_zero hX hY hZ).mpr h\n  rw [condMutualInfo_eq hX hY hZ] at hI\n  rw [entropy_assoc hX hY hZ, chain_rule _ (hX.prod_mk hY) hZ, chain_rule _ hX hZ, chain_rule _ hY hZ]\n  linarith [hI]\n\nend IsProbabilityMeasure\nend mutualInfo\nend ProbabilityTheory\n\nsection MeasureSpace_example\n\nopen ProbabilityTheory\n\nvariable {Ω S T : Type*} [MeasureSpace Ω] [IsProbabilityMeasure (ℙ : Measure Ω)]\n  [Fintype S] [Nonempty S] [MeasurableSpace S] [MeasurableSingletonClass S]\n  [Fintype T] [Nonempty T] [MeasurableSpace T] [MeasurableSingletonClass T]\n  {X : Ω → S} {Y : Ω → T}\n\n","proof":":= chain_rule _ hX hY","declId":"PFR.ForMathlib.Entropy.Basic.983_0.C2VYVgxGWlr8Qwd","decl":"/-- An example to illustrate how `MeasureSpace` can be used to suppress the ambient measure. -/\nexample (hX : Measurable X) (hY : Measurable Y) :\n  H[⟨X, Y⟩] = H[Y] + H[X | Y] "}
